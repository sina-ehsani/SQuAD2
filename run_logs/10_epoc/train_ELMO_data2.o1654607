2018-11-22 07:19:54 INFO     [Launching the SAN]
2018-11-22 07:19:54 INFO     [Loading data]
Loaded 130319 samples out of 130319
Loaded 11873 samples out of 11873
2018-11-22 07:20:23 INFO     [
############# Model Arch of SAN #############
DNetwork(
  (dropout): DropoutWrapper()
  (lexicon_encoder): LexiconEncoder(
    (dropout): DropoutWrapper()
    (dropout_emb): DropoutWrapper()
    (dropout_cove): DropoutWrapper()
    (embedding): Embedding(90953, 300, padding_idx=0)
    (ContextualEmbed): ContextualEmbed(
      (embedding): Embedding(90953, 300, padding_idx=0)
      (rnn1): LSTM(300, 300, bidirectional=True)
      (rnn2): LSTM(600, 300, bidirectional=True)
    )
    (prealign): AttentionWrapper(
      (score_func): SimilarityWrapper(
        (score_func): DotProductProject(
          (dropout): DropoutWrapper()
          (proj_1): Linear(in_features=300, out_features=128, bias=False)
          (proj_2): Linear(in_features=300, out_features=128, bias=False)
        )
      )
    )
    (pos_embedding): Embedding(54, 12, padding_idx=0)
    (ner_embedding): Embedding(41, 8, padding_idx=0)
    (doc_pwnn): PositionwiseNN(
      (w_0): Conv1d(1224, 256, kernel_size=(1,), stride=(1,))
      (w_1): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
      (dropout): DropoutWrapper()
    )
    (que_pwnn): PositionwiseNN(
      (w_0): Conv1d(900, 256, kernel_size=(1,), stride=(1,))
      (w_1): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
      (dropout): DropoutWrapper()
    )
  )
  (doc_encoder_low): OneLayerBRNN(
    (dropout): DropoutWrapper()
    (rnn): LSTM(856, 128, bidirectional=True)
  )
  (doc_encoder_high): OneLayerBRNN(
    (dropout): DropoutWrapper()
    (rnn): LSTM(856, 128, bidirectional=True)
  )
  (query_encoder_low): OneLayerBRNN(
    (dropout): DropoutWrapper()
    (rnn): LSTM(856, 128, bidirectional=True)
  )
  (query_encoder_high): OneLayerBRNN(
    (dropout): DropoutWrapper()
    (rnn): LSTM(856, 128, bidirectional=True)
  )
  (query_understand): OneLayerBRNN(
    (dropout): DropoutWrapper()
    (rnn): LSTM(512, 128, bidirectional=True)
  )
  (deep_attn): DeepAttentionWrapper(
    (dropout): DropoutWrapper()
    (attn_list): ModuleList(
      (0): AttentionWrapper(
        (score_func): SimilarityWrapper(
          (score_func): DotProductProject(
            (dropout): DropoutWrapper()
            (proj_1): Linear(in_features=1412, out_features=128, bias=False)
            (proj_2): Linear(in_features=1412, out_features=128, bias=False)
          )
        )
      )
      (1): AttentionWrapper(
        (score_func): SimilarityWrapper(
          (score_func): DotProductProject(
            (dropout): DropoutWrapper()
            (proj_1): Linear(in_features=1412, out_features=128, bias=False)
            (proj_2): Linear(in_features=1412, out_features=128, bias=False)
          )
        )
      )
      (2): AttentionWrapper(
        (score_func): SimilarityWrapper(
          (score_func): DotProductProject(
            (dropout): DropoutWrapper()
            (proj_1): Linear(in_features=1412, out_features=128, bias=False)
            (proj_2): Linear(in_features=1412, out_features=128, bias=False)
          )
        )
      )
    )
  )
  (doc_understand): OneLayerBRNN(
    (dropout): DropoutWrapper()
    (rnn): LSTM(1280, 128, bidirectional=True)
  )
  (doc_self_attn): AttentionWrapper(
    (score_func): SimilarityWrapper(
      (score_func): DotProductProject(
        (dropout): DropoutWrapper()
        (proj_1): Linear(in_features=2436, out_features=128, bias=False)
        (proj_2): Linear(in_features=2436, out_features=128, bias=False)
      )
    )
  )
  (doc_mem_gen): OneLayerBRNN(
    (dropout): DropoutWrapper()
    (rnn): LSTM(512, 128, bidirectional=True)
  )
  (query_sum_attn): SelfAttnWrapper(
    (att): LinearSelfAttn(
      (linear): Linear(in_features=256, out_features=1, bias=True)
      (dropout): DropoutWrapper()
    )
  )
  (decoder): SAN(
    (attn_b): FlatSimilarityWrapper(
      (att_dropout): DropoutWrapper()
      (score_func): BilinearFlatSim(
        (linear): Linear(in_features=256, out_features=256, bias=True)
        (dropout): DropoutWrapper()
      )
    )
    (attn_e): FlatSimilarityWrapper(
      (att_dropout): DropoutWrapper()
      (score_func): BilinearFlatSim(
        (linear): Linear(in_features=256, out_features=256, bias=True)
        (dropout): DropoutWrapper()
      )
    )
    (rnn): GRUCell(256, 256)
    (dropout): DropoutWrapper()
  )
  (doc_sum_attn): SelfAttnWrapper(
    (att): LinearSelfAttn(
      (linear): Linear(in_features=256, out_features=1, bias=True)
      (dropout): DropoutWrapper()
    )
  )
  (classifier): Classifier(
    (dropout): DropoutWrapper()
    (proj): Linear(in_features=512, out_features=1, bias=True)
  )
)
]
2018-11-22 07:20:23 INFO     [Total number of params: 9194805]
2018-11-22 07:20:36 WARNING  [At epoch 0]
2018-11-22 07:20:37 INFO     [#updates[     1] train loss[10.56736] remaining[0:20:19]]
2018-11-22 07:21:02 INFO     [#updates[   100] train loss[9.48453] remaining[0:16:49]]
2018-11-22 07:21:29 INFO     [#updates[   200] train loss[9.02485] remaining[0:16:53]]
2018-11-22 07:21:54 INFO     [#updates[   300] train loss[8.70576] remaining[0:16:12]]
2018-11-22 07:22:19 INFO     [#updates[   400] train loss[8.48540] remaining[0:15:42]]
2018-11-22 07:22:48 INFO     [#updates[   500] train loss[8.32949] remaining[0:15:39]]
2018-11-22 07:23:15 INFO     [#updates[   600] train loss[8.17943] remaining[0:15:18]]
2018-11-22 07:23:43 INFO     [#updates[   700] train loss[8.10242] remaining[0:14:59]]
2018-11-22 07:24:08 INFO     [#updates[   800] train loss[8.00074] remaining[0:14:27]]
2018-11-22 07:24:35 INFO     [#updates[   900] train loss[7.91639] remaining[0:14:00]]
2018-11-22 07:25:02 INFO     [#updates[  1000] train loss[7.83392] remaining[0:13:35]]
2018-11-22 07:25:22 INFO     [#updates[  1100] train loss[7.77315] remaining[0:12:51]]
2018-11-22 07:25:49 INFO     [#updates[  1200] train loss[7.70867] remaining[0:12:28]]
2018-11-22 07:26:14 INFO     [#updates[  1300] train loss[7.66244] remaining[0:12:00]]
2018-11-22 07:26:38 INFO     [#updates[  1400] train loss[7.58890] remaining[0:11:31]]
2018-11-22 07:27:06 INFO     [#updates[  1500] train loss[7.53751] remaining[0:11:09]]
2018-11-22 07:27:34 INFO     [#updates[  1600] train loss[7.49069] remaining[0:10:45]]
2018-11-22 07:27:58 INFO     [#updates[  1700] train loss[7.43750] remaining[0:10:16]]
2018-11-22 07:28:23 INFO     [#updates[  1800] train loss[7.39355] remaining[0:09:49]]
2018-11-22 07:28:48 INFO     [#updates[  1900] train loss[7.33840] remaining[0:09:21]]
2018-11-22 07:29:11 INFO     [#updates[  2000] train loss[7.28876] remaining[0:08:53]]
2018-11-22 07:29:37 INFO     [#updates[  2100] train loss[7.23279] remaining[0:08:27]]
2018-11-22 07:30:02 INFO     [#updates[  2200] train loss[7.17751] remaining[0:08:01]]
2018-11-22 07:30:27 INFO     [#updates[  2300] train loss[7.12824] remaining[0:07:35]]
2018-11-22 07:30:55 INFO     [#updates[  2400] train loss[7.07869] remaining[0:07:11]]
2018-11-22 07:31:21 INFO     [#updates[  2500] train loss[7.03701] remaining[0:06:45]]
2018-11-22 07:31:49 INFO     [#updates[  2600] train loss[6.98569] remaining[0:06:20]]
2018-11-22 07:32:14 INFO     [#updates[  2700] train loss[6.93742] remaining[0:05:54]]
2018-11-22 07:32:42 INFO     [#updates[  2800] train loss[6.89421] remaining[0:05:29]]
2018-11-22 07:33:09 INFO     [#updates[  2900] train loss[6.85081] remaining[0:05:04]]
2018-11-22 07:33:35 INFO     [#updates[  3000] train loss[6.80057] remaining[0:04:38]]
2018-11-22 07:34:02 INFO     [#updates[  3100] train loss[6.75407] remaining[0:04:12]]
2018-11-22 07:34:23 INFO     [#updates[  3200] train loss[6.70983] remaining[0:03:45]]
2018-11-22 07:34:43 INFO     [#updates[  3300] train loss[6.66126] remaining[0:03:18]]
2018-11-22 07:35:02 INFO     [#updates[  3400] train loss[6.61801] remaining[0:02:51]]
2018-11-22 07:35:22 INFO     [#updates[  3500] train loss[6.57954] remaining[0:02:25]]
2018-11-22 07:35:42 INFO     [#updates[  3600] train loss[6.53964] remaining[0:01:58]]
2018-11-22 07:36:02 INFO     [#updates[  3700] train loss[6.50235] remaining[0:01:33]]
2018-11-22 07:36:21 INFO     [#updates[  3800] train loss[6.46329] remaining[0:01:07]]
2018-11-22 07:36:41 INFO     [#updates[  3900] train loss[6.42792] remaining[0:00:42]]
2018-11-22 07:37:00 INFO     [#updates[  4000] train loss[6.39266] remaining[0:00:17]]
2018-11-22 07:37:59 INFO     [scheduler_type ms]
2018-11-22 07:38:12 INFO     [Saved the new best model and prediction]
2018-11-22 07:38:12 WARNING  [Epoch 0 - dev EM: 28.325 F1: 34.319 (best EM: 28.325 F1: 34.319)]
2018-11-22 07:38:12 WARNING  [Epoch 0 - ACC: 50.9223]
2018-11-22 07:38:12 WARNING  [Detailed Metric at Epoch 0: OrderedDict([('exact', 28.32477048766108), ('f1', 34.319010272740194), ('total', 11873), ('HasAns_exact', 56.73076923076923), ('HasAns_f1', 68.73643875982529), ('HasAns_total', 5928), ('NoAns_exact', 0.0), ('NoAns_f1', 0.0), ('NoAns_total', 5945)])]
2018-11-22 07:38:12 WARNING  [At epoch 1]
2018-11-22 07:38:12 INFO     [#updates[  4074] train loss[6.36476] remaining[0:12:23]]
2018-11-22 07:38:17 INFO     [#updates[  4100] train loss[6.35597] remaining[0:12:08]]
2018-11-22 07:38:36 INFO     [#updates[  4200] train loss[6.31509] remaining[0:12:35]]
2018-11-22 07:38:55 INFO     [#updates[  4300] train loss[6.28183] remaining[0:12:18]]
2018-11-22 07:39:14 INFO     [#updates[  4400] train loss[6.24775] remaining[0:11:55]]
2018-11-22 07:39:34 INFO     [#updates[  4500] train loss[6.21593] remaining[0:11:44]]
2018-11-22 07:39:53 INFO     [#updates[  4600] train loss[6.18713] remaining[0:11:20]]
2018-11-22 07:40:17 INFO     [#updates[  4700] train loss[6.15742] remaining[0:11:27]]
2018-11-22 07:40:44 INFO     [#updates[  4800] train loss[6.12804] remaining[0:11:38]]
2018-11-22 07:41:10 INFO     [#updates[  4900] train loss[6.09949] remaining[0:11:41]]
2018-11-22 07:41:36 INFO     [#updates[  5000] train loss[6.07132] remaining[0:11:32]]
2018-11-22 07:42:02 INFO     [#updates[  5100] train loss[6.04683] remaining[0:11:24]]
2018-11-22 07:42:29 INFO     [#updates[  5200] train loss[6.01685] remaining[0:11:13]]
2018-11-22 07:42:56 INFO     [#updates[  5300] train loss[5.99487] remaining[0:10:59]]
2018-11-22 07:43:20 INFO     [#updates[  5400] train loss[5.97070] remaining[0:10:38]]
2018-11-22 07:43:46 INFO     [#updates[  5500] train loss[5.94883] remaining[0:10:19]]
2018-11-22 07:44:12 INFO     [#updates[  5600] train loss[5.92602] remaining[0:10:00]]
2018-11-22 07:44:38 INFO     [#updates[  5700] train loss[5.90546] remaining[0:09:40]]
2018-11-22 07:44:58 INFO     [#updates[  5800] train loss[5.88334] remaining[0:09:12]]
2018-11-22 07:45:24 INFO     [#updates[  5900] train loss[5.85943] remaining[0:08:52]]
2018-11-22 07:45:49 INFO     [#updates[  6000] train loss[5.84059] remaining[0:08:29]]
2018-11-22 07:46:15 INFO     [#updates[  6100] train loss[5.82004] remaining[0:08:07]]
2018-11-22 07:46:42 INFO     [#updates[  6200] train loss[5.79847] remaining[0:07:47]]
2018-11-22 07:47:07 INFO     [#updates[  6300] train loss[5.77770] remaining[0:07:23]]
2018-11-22 07:47:34 INFO     [#updates[  6400] train loss[5.76093] remaining[0:07:01]]
2018-11-22 07:48:00 INFO     [#updates[  6500] train loss[5.74300] remaining[0:06:38]]
2018-11-22 07:48:27 INFO     [#updates[  6600] train loss[5.72552] remaining[0:06:16]]
2018-11-22 07:48:54 INFO     [#updates[  6700] train loss[5.70346] remaining[0:05:53]]
2018-11-22 07:49:21 INFO     [#updates[  6800] train loss[5.68642] remaining[0:05:30]]
2018-11-22 07:49:50 INFO     [#updates[  6900] train loss[5.66823] remaining[0:05:07]]
2018-11-22 07:50:18 INFO     [#updates[  7000] train loss[5.65022] remaining[0:04:44]]
2018-11-22 07:50:44 INFO     [#updates[  7100] train loss[5.63227] remaining[0:04:20]]
2018-11-22 07:51:10 INFO     [#updates[  7200] train loss[5.61690] remaining[0:03:55]]
2018-11-22 07:51:37 INFO     [#updates[  7300] train loss[5.60236] remaining[0:03:31]]
2018-11-22 07:52:03 INFO     [#updates[  7400] train loss[5.58601] remaining[0:03:06]]
2018-11-22 07:52:29 INFO     [#updates[  7500] train loss[5.56809] remaining[0:02:41]]
2018-11-22 07:52:56 INFO     [#updates[  7600] train loss[5.55296] remaining[0:02:16]]
2018-11-22 07:53:23 INFO     [#updates[  7700] train loss[5.53910] remaining[0:01:52]]
2018-11-22 07:53:51 INFO     [#updates[  7800] train loss[5.52414] remaining[0:01:27]]
2018-11-22 07:54:17 INFO     [#updates[  7900] train loss[5.50877] remaining[0:01:02]]
2018-11-22 07:54:46 INFO     [#updates[  8000] train loss[5.49708] remaining[0:00:36]]
2018-11-22 07:55:14 INFO     [#updates[  8100] train loss[5.48320] remaining[0:00:11]]
2018-11-22 07:56:11 INFO     [scheduler_type ms]
2018-11-22 07:56:24 INFO     [Saved the new best model and prediction]
2018-11-22 07:56:24 WARNING  [Epoch 1 - dev EM: 32.915 F1: 37.692 (best EM: 32.915 F1: 37.692)]
2018-11-22 07:56:24 WARNING  [Epoch 1 - ACC: 56.1610]
2018-11-22 07:56:24 WARNING  [Detailed Metric at Epoch 1: OrderedDict([('exact', 32.91501726606587), ('f1', 37.6922669703141), ('total', 11873), ('HasAns_exact', 65.90755735492577), ('HasAns_f1', 75.47575670353227), ('HasAns_total', 5928), ('NoAns_exact', 0.01682085786375105), ('NoAns_f1', 0.01682085786375105), ('NoAns_total', 5945)])]
2018-11-22 07:56:24 WARNING  [At epoch 2]
2018-11-22 07:56:24 INFO     [#updates[  8147] train loss[5.47619] remaining[0:09:57]]
2018-11-22 07:56:39 INFO     [#updates[  8200] train loss[5.46992] remaining[0:18:31]]
2018-11-22 07:57:06 INFO     [#updates[  8300] train loss[5.45489] remaining[0:17:47]]
2018-11-22 07:57:33 INFO     [#updates[  8400] train loss[5.43938] remaining[0:17:21]]
2018-11-22 07:58:00 INFO     [#updates[  8500] train loss[5.42402] remaining[0:16:51]]
2018-11-22 07:58:28 INFO     [#updates[  8600] train loss[5.41195] remaining[0:16:29]]
2018-11-22 07:58:54 INFO     [#updates[  8700] train loss[5.39686] remaining[0:15:56]]
2018-11-22 07:59:23 INFO     [#updates[  8800] train loss[5.38498] remaining[0:15:38]]
2018-11-22 07:59:52 INFO     [#updates[  8900] train loss[5.37198] remaining[0:15:17]]
2018-11-22 08:00:20 INFO     [#updates[  9000] train loss[5.35885] remaining[0:14:52]]
2018-11-22 08:00:51 INFO     [#updates[  9100] train loss[5.34546] remaining[0:14:32]]
2018-11-22 08:01:17 INFO     [#updates[  9200] train loss[5.33248] remaining[0:13:59]]
2018-11-22 08:01:44 INFO     [#updates[  9300] train loss[5.31886] remaining[0:13:30]]
2018-11-22 08:02:13 INFO     [#updates[  9400] train loss[5.30584] remaining[0:13:05]]
2018-11-22 08:02:43 INFO     [#updates[  9500] train loss[5.29248] remaining[0:12:41]]
2018-11-22 08:03:12 INFO     [#updates[  9600] train loss[5.28140] remaining[0:12:15]]
2018-11-22 08:03:40 INFO     [#updates[  9700] train loss[5.26945] remaining[0:11:47]]
2018-11-22 08:04:07 INFO     [#updates[  9800] train loss[5.25685] remaining[0:11:18]]
2018-11-22 08:04:36 INFO     [#updates[  9900] train loss[5.24407] remaining[0:10:51]]
2018-11-22 08:05:03 INFO     [#updates[ 10000] train loss[5.23453] remaining[0:10:21]]
2018-11-22 08:05:28 INFO     [#updates[ 10100] train loss[5.22490] remaining[0:09:50]]
2018-11-22 08:05:56 INFO     [#updates[ 10200] train loss[5.21452] remaining[0:09:22]]
2018-11-22 08:06:24 INFO     [#updates[ 10300] train loss[5.20375] remaining[0:08:55]]
2018-11-22 08:06:51 INFO     [#updates[ 10400] train loss[5.19323] remaining[0:08:26]]
2018-11-22 08:07:17 INFO     [#updates[ 10500] train loss[5.18267] remaining[0:07:56]]
2018-11-22 08:07:45 INFO     [#updates[ 10600] train loss[5.17192] remaining[0:07:29]]
2018-11-22 08:08:10 INFO     [#updates[ 10700] train loss[5.16119] remaining[0:07:00]]
2018-11-22 08:08:37 INFO     [#updates[ 10800] train loss[5.15200] remaining[0:06:32]]
2018-11-22 08:09:04 INFO     [#updates[ 10900] train loss[5.14296] remaining[0:06:04]]
2018-11-22 08:09:32 INFO     [#updates[ 11000] train loss[5.13248] remaining[0:05:36]]
2018-11-22 08:10:00 INFO     [#updates[ 11100] train loss[5.12155] remaining[0:05:09]]
2018-11-22 08:10:22 INFO     [#updates[ 11200] train loss[5.11222] remaining[0:04:39]]
2018-11-22 08:10:51 INFO     [#updates[ 11300] train loss[5.10227] remaining[0:04:12]]
2018-11-22 08:11:20 INFO     [#updates[ 11400] train loss[5.09379] remaining[0:03:45]]
2018-11-22 08:11:48 INFO     [#updates[ 11500] train loss[5.08526] remaining[0:03:18]]
2018-11-22 08:12:16 INFO     [#updates[ 11600] train loss[5.07627] remaining[0:02:50]]
2018-11-22 08:12:43 INFO     [#updates[ 11700] train loss[5.06809] remaining[0:02:23]]
2018-11-22 08:13:10 INFO     [#updates[ 11800] train loss[5.06193] remaining[0:01:55]]
2018-11-22 08:13:37 INFO     [#updates[ 11900] train loss[5.05489] remaining[0:01:27]]
2018-11-22 08:14:06 INFO     [#updates[ 12000] train loss[5.04491] remaining[0:01:00]]
2018-11-22 08:14:34 INFO     [#updates[ 12100] train loss[5.03675] remaining[0:00:32]]
2018-11-22 08:15:03 INFO     [#updates[ 12200] train loss[5.02796] remaining[0:00:05]]
2018-11-22 08:15:53 INFO     [scheduler_type ms]
2018-11-22 08:16:05 INFO     [Saved the new best model and prediction]
2018-11-22 08:16:05 WARNING  [Epoch 2 - dev EM: 33.833 F1: 38.939 (best EM: 33.833 F1: 38.939)]
2018-11-22 08:16:05 WARNING  [Epoch 2 - ACC: 56.1442]
2018-11-22 08:16:05 WARNING  [Detailed Metric at Epoch 2: OrderedDict([('exact', 33.83306662174682), ('f1', 38.938690483747685), ('total', 11873), ('HasAns_exact', 67.76315789473684), ('HasAns_f1', 77.98904725262084), ('HasAns_total', 5928), ('NoAns_exact', 0.0), ('NoAns_f1', 0.0), ('NoAns_total', 5945)])]
2018-11-22 08:16:05 WARNING  [At epoch 3]
2018-11-22 08:16:05 INFO     [#updates[ 12220] train loss[5.02628] remaining[0:10:49]]
2018-11-22 08:16:26 INFO     [#updates[ 12300] train loss[5.01963] remaining[0:17:24]]
2018-11-22 08:16:53 INFO     [#updates[ 12400] train loss[5.01211] remaining[0:17:11]]
2018-11-22 08:17:20 INFO     [#updates[ 12500] train loss[5.00285] remaining[0:16:49]]
2018-11-22 08:17:47 INFO     [#updates[ 12600] train loss[4.99368] remaining[0:16:27]]
2018-11-22 08:18:15 INFO     [#updates[ 12700] train loss[4.98491] remaining[0:16:06]]
2018-11-22 08:18:41 INFO     [#updates[ 12800] train loss[4.97556] remaining[0:15:38]]
2018-11-22 08:19:09 INFO     [#updates[ 12900] train loss[4.96675] remaining[0:15:13]]
2018-11-22 08:19:39 INFO     [#updates[ 13000] train loss[4.95743] remaining[0:15:00]]
2018-11-22 08:20:03 INFO     [#updates[ 13100] train loss[4.95004] remaining[0:14:20]]
2018-11-22 08:20:31 INFO     [#updates[ 13200] train loss[4.94331] remaining[0:13:56]]
2018-11-22 08:20:57 INFO     [#updates[ 13300] train loss[4.93572] remaining[0:13:28]]
2018-11-22 08:21:26 INFO     [#updates[ 13400] train loss[4.92775] remaining[0:13:04]]
2018-11-22 08:21:54 INFO     [#updates[ 13500] train loss[4.92104] remaining[0:12:39]]
2018-11-22 08:22:21 INFO     [#updates[ 13600] train loss[4.91270] remaining[0:12:12]]
2018-11-22 08:22:48 INFO     [#updates[ 13700] train loss[4.90458] remaining[0:11:44]]
2018-11-22 08:23:16 INFO     [#updates[ 13800] train loss[4.89711] remaining[0:11:19]]
2018-11-22 08:23:42 INFO     [#updates[ 13900] train loss[4.88913] remaining[0:10:50]]
2018-11-22 08:24:10 INFO     [#updates[ 14000] train loss[4.88299] remaining[0:10:23]]
2018-11-22 08:24:36 INFO     [#updates[ 14100] train loss[4.87571] remaining[0:09:54]]
2018-11-22 08:25:02 INFO     [#updates[ 14200] train loss[4.86883] remaining[0:09:26]]
2018-11-22 08:25:28 INFO     [#updates[ 14300] train loss[4.86137] remaining[0:08:58]]
2018-11-22 08:25:56 INFO     [#updates[ 14400] train loss[4.85493] remaining[0:08:32]]
2018-11-22 08:26:24 INFO     [#updates[ 14500] train loss[4.84657] remaining[0:08:05]]
2018-11-22 08:26:52 INFO     [#updates[ 14600] train loss[4.84247] remaining[0:07:39]]
2018-11-22 08:27:19 INFO     [#updates[ 14700] train loss[4.83556] remaining[0:07:12]]
2018-11-22 08:27:48 INFO     [#updates[ 14800] train loss[4.82877] remaining[0:06:46]]
2018-11-22 08:28:14 INFO     [#updates[ 14900] train loss[4.82198] remaining[0:06:18]]
2018-11-22 08:28:43 INFO     [#updates[ 15000] train loss[4.81552] remaining[0:05:52]]
2018-11-22 08:29:12 INFO     [#updates[ 15100] train loss[4.80923] remaining[0:05:25]]
2018-11-22 08:29:42 INFO     [#updates[ 15200] train loss[4.80302] remaining[0:04:59]]
2018-11-22 08:30:09 INFO     [#updates[ 15300] train loss[4.79561] remaining[0:04:31]]
2018-11-22 08:30:40 INFO     [#updates[ 15400] train loss[4.78986] remaining[0:04:05]]
2018-11-22 08:31:08 INFO     [#updates[ 15500] train loss[4.78403] remaining[0:03:37]]
2018-11-22 08:31:38 INFO     [#updates[ 15600] train loss[4.77737] remaining[0:03:10]]
2018-11-22 08:32:06 INFO     [#updates[ 15700] train loss[4.77101] remaining[0:02:43]]
2018-11-22 08:32:33 INFO     [#updates[ 15800] train loss[4.76473] remaining[0:02:15]]
2018-11-22 08:33:01 INFO     [#updates[ 15900] train loss[4.75973] remaining[0:01:48]]
2018-11-22 08:33:31 INFO     [#updates[ 16000] train loss[4.75276] remaining[0:01:20]]
2018-11-22 08:34:00 INFO     [#updates[ 16100] train loss[4.74768] remaining[0:00:53]]
2018-11-22 08:34:29 INFO     [#updates[ 16200] train loss[4.74262] remaining[0:00:25]]
2018-11-22 08:35:40 INFO     [scheduler_type ms]
2018-11-22 08:35:50 INFO     [Saved the new best model and prediction]
2018-11-22 08:35:50 WARNING  [Epoch 3 - dev EM: 35.290 F1: 39.947 (best EM: 35.290 F1: 39.947)]
2018-11-22 08:35:50 WARNING  [Epoch 3 - ACC: 61.0461]
2018-11-22 08:35:50 WARNING  [Detailed Metric at Epoch 3: OrderedDict([('exact', 35.2901541312221), ('f1', 39.947172434628364), ('total', 11873), ('HasAns_exact', 70.68151147098516), ('HasAns_f1', 80.00890322475415), ('HasAns_total', 5928), ('NoAns_exact', 0.0), ('NoAns_f1', 0.0), ('NoAns_total', 5945)])]
2018-11-22 08:35:50 WARNING  [At epoch 4]
2018-11-22 08:35:50 INFO     [#updates[ 16293] train loss[4.73855] remaining[0:16:30]]
2018-11-22 08:35:52 INFO     [#updates[ 16300] train loss[4.73808] remaining[0:20:26]]
2018-11-22 08:36:19 INFO     [#updates[ 16400] train loss[4.73168] remaining[0:18:03]]
2018-11-22 08:36:47 INFO     [#updates[ 16500] train loss[4.72612] remaining[0:17:46]]
2018-11-22 08:37:14 INFO     [#updates[ 16600] train loss[4.72186] remaining[0:17:10]]
2018-11-22 08:37:43 INFO     [#updates[ 16700] train loss[4.71462] remaining[0:16:57]]
2018-11-22 08:38:11 INFO     [#updates[ 16800] train loss[4.70901] remaining[0:16:28]]
2018-11-22 08:38:38 INFO     [#updates[ 16900] train loss[4.70245] remaining[0:15:59]]
2018-11-22 08:39:08 INFO     [#updates[ 17000] train loss[4.69813] remaining[0:15:43]]
2018-11-22 08:39:38 INFO     [#updates[ 17100] train loss[4.69335] remaining[0:15:23]]
2018-11-22 08:40:04 INFO     [#updates[ 17200] train loss[4.68785] remaining[0:14:46]]
2018-11-22 08:40:33 INFO     [#updates[ 17300] train loss[4.68068] remaining[0:14:20]]
2018-11-22 08:41:02 INFO     [#updates[ 17400] train loss[4.67522] remaining[0:13:54]]
2018-11-22 08:41:27 INFO     [#updates[ 17500] train loss[4.66825] remaining[0:13:19]]
2018-11-22 08:41:55 INFO     [#updates[ 17600] train loss[4.66238] remaining[0:12:51]]
2018-11-22 08:42:21 INFO     [#updates[ 17700] train loss[4.65730] remaining[0:12:20]]
2018-11-22 08:42:49 INFO     [#updates[ 17800] train loss[4.65166] remaining[0:11:52]]
2018-11-22 08:43:17 INFO     [#updates[ 17900] train loss[4.64596] remaining[0:11:25]]
2018-11-22 08:43:45 INFO     [#updates[ 18000] train loss[4.63963] remaining[0:10:57]]
2018-11-22 08:44:12 INFO     [#updates[ 18100] train loss[4.63372] remaining[0:10:29]]
2018-11-22 08:44:40 INFO     [#updates[ 18200] train loss[4.62880] remaining[0:10:01]]
2018-11-22 08:45:07 INFO     [#updates[ 18300] train loss[4.62492] remaining[0:09:32]]
2018-11-22 08:45:36 INFO     [#updates[ 18400] train loss[4.62029] remaining[0:09:06]]
2018-11-22 08:46:03 INFO     [#updates[ 18500] train loss[4.61568] remaining[0:08:38]]
2018-11-22 08:46:30 INFO     [#updates[ 18600] train loss[4.61038] remaining[0:08:09]]
2018-11-22 08:46:58 INFO     [#updates[ 18700] train loss[4.60593] remaining[0:07:42]]
2018-11-22 08:47:28 INFO     [#updates[ 18800] train loss[4.60039] remaining[0:07:15]]
2018-11-22 08:47:58 INFO     [#updates[ 18900] train loss[4.59574] remaining[0:06:49]]
2018-11-22 08:48:24 INFO     [#updates[ 19000] train loss[4.59129] remaining[0:06:20]]
2018-11-22 08:48:51 INFO     [#updates[ 19100] train loss[4.58703] remaining[0:05:52]]
2018-11-22 08:49:18 INFO     [#updates[ 19200] train loss[4.58254] remaining[0:05:23]]
2018-11-22 08:49:47 INFO     [#updates[ 19300] train loss[4.57727] remaining[0:04:56]]
2018-11-22 08:50:14 INFO     [#updates[ 19400] train loss[4.57382] remaining[0:04:28]]
2018-11-22 08:50:41 INFO     [#updates[ 19500] train loss[4.56980] remaining[0:04:00]]
2018-11-22 08:51:06 INFO     [#updates[ 19600] train loss[4.56490] remaining[0:03:31]]
2018-11-22 08:51:34 INFO     [#updates[ 19700] train loss[4.56026] remaining[0:03:04]]
2018-11-22 08:52:01 INFO     [#updates[ 19800] train loss[4.55584] remaining[0:02:36]]
2018-11-22 08:52:27 INFO     [#updates[ 19900] train loss[4.55216] remaining[0:02:08]]
2018-11-22 08:52:55 INFO     [#updates[ 20000] train loss[4.54887] remaining[0:01:40]]
2018-11-22 08:53:24 INFO     [#updates[ 20100] train loss[4.54412] remaining[0:01:13]]
2018-11-22 08:53:51 INFO     [#updates[ 20200] train loss[4.53890] remaining[0:00:45]]
2018-11-22 08:54:18 INFO     [#updates[ 20300] train loss[4.53486] remaining[0:00:17]]
2018-11-22 08:55:22 INFO     [scheduler_type ms]
2018-11-22 08:55:29 INFO     [Saved the new best model and prediction]
2018-11-22 08:55:29 WARNING  [Epoch 4 - dev EM: 35.627 F1: 40.342 (best EM: 35.627 F1: 40.342)]
2018-11-22 08:55:29 WARNING  [Epoch 4 - ACC: 60.6586]
2018-11-22 08:55:29 WARNING  [Detailed Metric at Epoch 4: OrderedDict([('exact', 35.62705297734355), ('f1', 40.34238499322831), ('total', 11873), ('HasAns_exact', 71.35627530364373), ('HasAns_f1', 80.80046171130225), ('HasAns_total', 5928), ('NoAns_exact', 0.0), ('NoAns_f1', 0.0), ('NoAns_total', 5945)])]
2018-11-22 08:55:29 WARNING  [At epoch 5]
2018-11-22 08:55:30 INFO     [#updates[ 20366] train loss[4.53287] remaining[0:13:54]]
2018-11-22 08:55:40 INFO     [#updates[ 20400] train loss[4.53120] remaining[0:19:53]]
2018-11-22 08:56:06 INFO     [#updates[ 20500] train loss[4.52677] remaining[0:17:54]]
2018-11-22 08:56:28 INFO     [#updates[ 20600] train loss[4.52193] remaining[0:15:56]]
2018-11-22 08:56:54 INFO     [#updates[ 20700] train loss[4.51766] remaining[0:15:46]]
2018-11-22 08:57:22 INFO     [#updates[ 20800] train loss[4.51283] remaining[0:15:41]]
2018-11-22 08:57:48 INFO     [#updates[ 20900] train loss[4.50787] remaining[0:15:20]]
2018-11-22 08:58:14 INFO     [#updates[ 21000] train loss[4.50278] remaining[0:14:52]]
2018-11-22 08:58:44 INFO     [#updates[ 21100] train loss[4.49887] remaining[0:14:44]]
2018-11-22 08:59:14 INFO     [#updates[ 21200] train loss[4.49405] remaining[0:14:29]]
2018-11-22 08:59:42 INFO     [#updates[ 21300] train loss[4.49045] remaining[0:14:08]]
2018-11-22 09:00:10 INFO     [#updates[ 21400] train loss[4.48615] remaining[0:13:44]]
2018-11-22 09:00:40 INFO     [#updates[ 21500] train loss[4.48255] remaining[0:13:23]]
2018-11-22 09:01:07 INFO     [#updates[ 21600] train loss[4.47841] remaining[0:12:56]]
2018-11-22 09:01:34 INFO     [#updates[ 21700] train loss[4.47489] remaining[0:12:27]]
2018-11-22 09:02:01 INFO     [#updates[ 21800] train loss[4.47173] remaining[0:11:59]]
2018-11-22 09:02:30 INFO     [#updates[ 21900] train loss[4.46779] remaining[0:11:36]]
2018-11-22 09:02:59 INFO     [#updates[ 22000] train loss[4.46336] remaining[0:11:10]]
2018-11-22 09:03:27 INFO     [#updates[ 22100] train loss[4.45971] remaining[0:10:43]]
2018-11-22 09:03:55 INFO     [#updates[ 22200] train loss[4.45555] remaining[0:10:16]]
2018-11-22 09:04:23 INFO     [#updates[ 22300] train loss[4.45117] remaining[0:09:49]]
2018-11-22 09:04:51 INFO     [#updates[ 22400] train loss[4.44749] remaining[0:09:22]]
2018-11-22 09:05:18 INFO     [#updates[ 22500] train loss[4.44384] remaining[0:08:54]]
2018-11-22 09:05:48 INFO     [#updates[ 22600] train loss[4.43989] remaining[0:08:28]]
2018-11-22 09:06:15 INFO     [#updates[ 22700] train loss[4.43675] remaining[0:08:00]]
2018-11-22 09:06:43 INFO     [#updates[ 22800] train loss[4.43325] remaining[0:07:33]]
2018-11-22 09:07:11 INFO     [#updates[ 22900] train loss[4.42953] remaining[0:07:05]]
2018-11-22 09:07:42 INFO     [#updates[ 23000] train loss[4.42575] remaining[0:06:39]]
2018-11-22 09:08:08 INFO     [#updates[ 23100] train loss[4.42258] remaining[0:06:10]]
2018-11-22 09:08:38 INFO     [#updates[ 23200] train loss[4.41880] remaining[0:05:44]]
2018-11-22 09:09:06 INFO     [#updates[ 23300] train loss[4.41449] remaining[0:05:16]]
2018-11-22 09:09:35 INFO     [#updates[ 23400] train loss[4.41085] remaining[0:04:49]]
2018-11-22 09:10:03 INFO     [#updates[ 23500] train loss[4.40744] remaining[0:04:21]]
2018-11-22 09:10:33 INFO     [#updates[ 23600] train loss[4.40333] remaining[0:03:54]]
2018-11-22 09:11:02 INFO     [#updates[ 23700] train loss[4.40028] remaining[0:03:26]]
2018-11-22 09:11:32 INFO     [#updates[ 23800] train loss[4.39637] remaining[0:02:58]]
2018-11-22 09:11:59 INFO     [#updates[ 23900] train loss[4.39219] remaining[0:02:30]]
2018-11-22 09:12:26 INFO     [#updates[ 24000] train loss[4.38857] remaining[0:02:02]]
2018-11-22 09:12:53 INFO     [#updates[ 24100] train loss[4.38415] remaining[0:01:34]]
2018-11-22 09:13:21 INFO     [#updates[ 24200] train loss[4.38123] remaining[0:01:06]]
2018-11-22 09:13:47 INFO     [#updates[ 24300] train loss[4.37816] remaining[0:00:38]]
2018-11-22 09:14:15 INFO     [#updates[ 24400] train loss[4.37451] remaining[0:00:10]]
2018-11-22 09:15:13 INFO     [scheduler_type ms]
2018-11-22 09:15:20 INFO     [Saved the new best model and prediction]
2018-11-22 09:15:20 WARNING  [Epoch 5 - dev EM: 36.402 F1: 40.604 (best EM: 36.402 F1: 40.604)]
2018-11-22 09:15:20 WARNING  [Epoch 5 - ACC: 57.0117]
2018-11-22 09:15:20 WARNING  [Detailed Metric at Epoch 5: OrderedDict([('exact', 36.40192032342289), ('f1', 40.60424663192623), ('total', 11873), ('HasAns_exact', 72.89136302294197), ('HasAns_f1', 81.30806684562417), ('HasAns_total', 5928), ('NoAns_exact', 0.01682085786375105), ('NoAns_f1', 0.01682085786375105), ('NoAns_total', 5945)])]
2018-11-22 09:15:20 WARNING  [At epoch 6]
2018-11-22 09:15:20 INFO     [#updates[ 24439] train loss[4.37325] remaining[0:17:51]]
2018-11-22 09:15:36 INFO     [#updates[ 24500] train loss[4.37063] remaining[0:17:35]]
2018-11-22 09:16:01 INFO     [#updates[ 24600] train loss[4.36664] remaining[0:16:32]]
2018-11-22 09:16:29 INFO     [#updates[ 24700] train loss[4.36278] remaining[0:16:45]]
2018-11-22 09:16:55 INFO     [#updates[ 24800] train loss[4.35955] remaining[0:16:19]]
2018-11-22 09:17:24 INFO     [#updates[ 24900] train loss[4.35587] remaining[0:16:12]]
2018-11-22 09:17:51 INFO     [#updates[ 25000] train loss[4.35214] remaining[0:15:43]]
2018-11-22 09:18:18 INFO     [#updates[ 25100] train loss[4.34859] remaining[0:15:19]]
2018-11-22 09:18:48 INFO     [#updates[ 25200] train loss[4.34547] remaining[0:15:04]]
2018-11-22 09:19:18 INFO     [#updates[ 25300] train loss[4.34257] remaining[0:14:48]]
2018-11-22 09:19:46 INFO     [#updates[ 25400] train loss[4.33986] remaining[0:14:20]]
2018-11-22 09:20:16 INFO     [#updates[ 25500] train loss[4.33605] remaining[0:14:01]]
2018-11-22 09:20:46 INFO     [#updates[ 25600] train loss[4.33253] remaining[0:13:37]]
2018-11-22 09:21:13 INFO     [#updates[ 25700] train loss[4.32910] remaining[0:13:07]]
2018-11-22 09:21:41 INFO     [#updates[ 25800] train loss[4.32611] remaining[0:12:39]]
2018-11-22 09:22:11 INFO     [#updates[ 25900] train loss[4.32244] remaining[0:12:14]]
2018-11-22 09:22:40 INFO     [#updates[ 26000] train loss[4.31885] remaining[0:11:47]]
2018-11-22 09:23:08 INFO     [#updates[ 26100] train loss[4.31550] remaining[0:11:19]]
2018-11-22 09:23:34 INFO     [#updates[ 26200] train loss[4.31267] remaining[0:10:48]]
2018-11-22 09:24:03 INFO     [#updates[ 26300] train loss[4.30974] remaining[0:10:22]]
2018-11-22 09:24:30 INFO     [#updates[ 26400] train loss[4.30691] remaining[0:09:52]]
2018-11-22 09:24:56 INFO     [#updates[ 26500] train loss[4.30352] remaining[0:09:22]]
2018-11-22 09:25:25 INFO     [#updates[ 26600] train loss[4.30048] remaining[0:08:54]]
2018-11-22 09:25:55 INFO     [#updates[ 26700] train loss[4.29695] remaining[0:08:29]]
2018-11-22 09:26:23 INFO     [#updates[ 26800] train loss[4.29435] remaining[0:08:00]]
2018-11-22 09:26:51 INFO     [#updates[ 26900] train loss[4.29193] remaining[0:07:32]]
2018-11-22 09:27:17 INFO     [#updates[ 27000] train loss[4.28938] remaining[0:07:03]]
2018-11-22 09:27:47 INFO     [#updates[ 27100] train loss[4.28657] remaining[0:06:36]]
2018-11-22 09:28:14 INFO     [#updates[ 27200] train loss[4.28327] remaining[0:06:07]]
2018-11-22 09:28:44 INFO     [#updates[ 27300] train loss[4.28040] remaining[0:05:40]]
2018-11-22 09:29:12 INFO     [#updates[ 27400] train loss[4.27683] remaining[0:05:12]]
2018-11-22 09:29:40 INFO     [#updates[ 27500] train loss[4.27343] remaining[0:04:44]]
2018-11-22 09:30:05 INFO     [#updates[ 27600] train loss[4.27023] remaining[0:04:14]]
2018-11-22 09:30:26 INFO     [#updates[ 27700] train loss[4.26732] remaining[0:03:45]]
2018-11-22 09:30:54 INFO     [#updates[ 27800] train loss[4.26431] remaining[0:03:17]]
2018-11-22 09:31:20 INFO     [#updates[ 27900] train loss[4.26071] remaining[0:02:49]]
2018-11-22 09:31:49 INFO     [#updates[ 28000] train loss[4.25821] remaining[0:02:21]]
2018-11-22 09:32:16 INFO     [#updates[ 28100] train loss[4.25495] remaining[0:01:54]]
2018-11-22 09:32:43 INFO     [#updates[ 28200] train loss[4.25213] remaining[0:01:26]]
2018-11-22 09:33:09 INFO     [#updates[ 28300] train loss[4.24899] remaining[0:00:58]]
2018-11-22 09:33:35 INFO     [#updates[ 28400] train loss[4.24621] remaining[0:00:30]]
2018-11-22 09:34:02 INFO     [#updates[ 28500] train loss[4.24350] remaining[0:00:03]]
2018-11-22 09:34:51 INFO     [scheduler_type ms]
2018-11-22 09:34:57 INFO     [Saved the new best model and prediction]
2018-11-22 09:34:57 WARNING  [Epoch 6 - dev EM: 36.714 F1: 40.936 (best EM: 36.714 F1: 40.936)]
2018-11-22 09:34:57 WARNING  [Epoch 6 - ACC: 57.7192]
2018-11-22 09:34:57 WARNING  [Detailed Metric at Epoch 6: OrderedDict([('exact', 36.71355175608524), ('f1', 40.93593751014264), ('total', 11873), ('HasAns_exact', 73.51551956815115), ('HasAns_f1', 81.97239980734201), ('HasAns_total', 5928), ('NoAns_exact', 0.01682085786375105), ('NoAns_f1', 0.01682085786375105), ('NoAns_total', 5945)])]
2018-11-22 09:34:57 WARNING  [At epoch 7]
2018-11-22 09:34:57 INFO     [#updates[ 28512] train loss[4.24303] remaining[0:16:59]]
2018-11-22 09:35:19 INFO     [#updates[ 28600] train loss[4.24020] remaining[0:16:16]]
2018-11-22 09:35:47 INFO     [#updates[ 28700] train loss[4.23678] remaining[0:17:10]]
2018-11-22 09:36:10 INFO     [#updates[ 28800] train loss[4.23478] remaining[0:15:53]]
2018-11-22 09:36:36 INFO     [#updates[ 28900] train loss[4.23232] remaining[0:15:39]]
2018-11-22 09:37:03 INFO     [#updates[ 29000] train loss[4.22929] remaining[0:15:25]]
2018-11-22 09:37:31 INFO     [#updates[ 29100] train loss[4.22592] remaining[0:15:11]]
2018-11-22 09:37:56 INFO     [#updates[ 29200] train loss[4.22303] remaining[0:14:40]]
2018-11-22 09:38:22 INFO     [#updates[ 29300] train loss[4.22043] remaining[0:14:11]]
2018-11-22 09:38:48 INFO     [#updates[ 29400] train loss[4.21707] remaining[0:13:45]]
2018-11-22 09:39:17 INFO     [#updates[ 29500] train loss[4.21393] remaining[0:13:31]]
2018-11-22 09:39:45 INFO     [#updates[ 29600] train loss[4.21052] remaining[0:13:08]]
2018-11-22 09:40:07 INFO     [#updates[ 29700] train loss[4.20783] remaining[0:12:32]]
2018-11-22 09:40:33 INFO     [#updates[ 29800] train loss[4.20482] remaining[0:12:06]]
2018-11-22 09:40:59 INFO     [#updates[ 29900] train loss[4.20194] remaining[0:11:39]]
2018-11-22 09:41:26 INFO     [#updates[ 30000] train loss[4.19877] remaining[0:11:15]]
2018-11-22 09:41:54 INFO     [#updates[ 30100] train loss[4.19609] remaining[0:10:51]]
2018-11-22 09:42:19 INFO     [#updates[ 30200] train loss[4.19359] remaining[0:10:24]]
2018-11-22 09:42:47 INFO     [#updates[ 30300] train loss[4.19063] remaining[0:10:00]]
2018-11-22 09:43:13 INFO     [#updates[ 30400] train loss[4.18812] remaining[0:09:33]]
2018-11-22 09:43:40 INFO     [#updates[ 30500] train loss[4.18606] remaining[0:09:07]]
2018-11-22 09:44:07 INFO     [#updates[ 30600] train loss[4.18379] remaining[0:08:42]]
2018-11-22 09:44:29 INFO     [#updates[ 30700] train loss[4.18145] remaining[0:08:12]]
2018-11-22 09:44:52 INFO     [#updates[ 30800] train loss[4.17906] remaining[0:07:44]]
2018-11-22 09:45:21 INFO     [#updates[ 30900] train loss[4.17624] remaining[0:07:20]]
2018-11-22 09:45:49 INFO     [#updates[ 31000] train loss[4.17334] remaining[0:06:54]]
2018-11-22 09:46:14 INFO     [#updates[ 31100] train loss[4.17037] remaining[0:06:28]]
2018-11-22 09:46:41 INFO     [#updates[ 31200] train loss[4.16752] remaining[0:06:02]]
2018-11-22 09:47:07 INFO     [#updates[ 31300] train loss[4.16454] remaining[0:05:36]]
2018-11-22 09:47:35 INFO     [#updates[ 31400] train loss[4.16201] remaining[0:05:10]]
2018-11-22 09:48:02 INFO     [#updates[ 31500] train loss[4.15983] remaining[0:04:44]]
2018-11-22 09:48:27 INFO     [#updates[ 31600] train loss[4.15737] remaining[0:04:18]]
2018-11-22 09:48:53 INFO     [#updates[ 31700] train loss[4.15446] remaining[0:03:51]]
2018-11-22 09:49:20 INFO     [#updates[ 31800] train loss[4.15234] remaining[0:03:25]]
2018-11-22 09:49:47 INFO     [#updates[ 31900] train loss[4.15018] remaining[0:02:59]]
2018-11-22 09:50:14 INFO     [#updates[ 32000] train loss[4.14706] remaining[0:02:33]]
2018-11-22 09:50:42 INFO     [#updates[ 32100] train loss[4.14441] remaining[0:02:07]]
2018-11-22 09:51:08 INFO     [#updates[ 32200] train loss[4.14207] remaining[0:01:41]]
2018-11-22 09:51:34 INFO     [#updates[ 32300] train loss[4.13998] remaining[0:01:14]]
2018-11-22 09:52:01 INFO     [#updates[ 32400] train loss[4.13844] remaining[0:00:48]]
2018-11-22 09:52:27 INFO     [#updates[ 32500] train loss[4.13553] remaining[0:00:22]]
2018-11-22 09:53:34 INFO     [scheduler_type ms]
2018-11-22 09:53:37 WARNING  [Epoch 7 - dev EM: 36.638 F1: 40.957 (best EM: 36.714 F1: 40.936)]
2018-11-22 09:53:37 WARNING  [Epoch 7 - ACC: 61.9641]
2018-11-22 09:53:37 WARNING  [Detailed Metric at Epoch 7: OrderedDict([('exact', 36.63774951570791), ('f1', 40.95697370733129), ('total', 11873), ('HasAns_exact', 73.38056680161944), ('HasAns_f1', 82.0314016240122), ('HasAns_total', 5928), ('NoAns_exact', 0.0), ('NoAns_f1', 0.0), ('NoAns_total', 5945)])]
2018-11-22 09:53:37 WARNING  [At epoch 8]
2018-11-22 09:53:37 INFO     [#updates[ 32585] train loss[4.13366] remaining[0:11:31]]
2018-11-22 09:53:41 INFO     [#updates[ 32600] train loss[4.13316] remaining[0:16:25]]
2018-11-22 09:54:04 INFO     [#updates[ 32700] train loss[4.13037] remaining[0:15:48]]
2018-11-22 09:54:29 INFO     [#updates[ 32800] train loss[4.12746] remaining[0:15:34]]
2018-11-22 09:54:56 INFO     [#updates[ 32900] train loss[4.12432] remaining[0:15:46]]
2018-11-22 09:55:22 INFO     [#updates[ 33000] train loss[4.12169] remaining[0:15:27]]
2018-11-22 09:55:50 INFO     [#updates[ 33100] train loss[4.11965] remaining[0:15:18]]
2018-11-22 09:56:18 INFO     [#updates[ 33200] train loss[4.11724] remaining[0:15:06]]
2018-11-22 09:56:45 INFO     [#updates[ 33300] train loss[4.11532] remaining[0:14:44]]
2018-11-22 09:57:12 INFO     [#updates[ 33400] train loss[4.11270] remaining[0:14:20]]
2018-11-22 09:57:40 INFO     [#updates[ 33500] train loss[4.10978] remaining[0:13:59]]
2018-11-22 09:58:06 INFO     [#updates[ 33600] train loss[4.10676] remaining[0:13:31]]
2018-11-22 09:58:33 INFO     [#updates[ 33700] train loss[4.10417] remaining[0:13:04]]
2018-11-22 09:59:00 INFO     [#updates[ 33800] train loss[4.10168] remaining[0:12:39]]
2018-11-22 09:59:26 INFO     [#updates[ 33900] train loss[4.09926] remaining[0:12:11]]
2018-11-22 09:59:54 INFO     [#updates[ 34000] train loss[4.09682] remaining[0:11:49]]
2018-11-22 10:00:22 INFO     [#updates[ 34100] train loss[4.09409] remaining[0:11:24]]
2018-11-22 10:00:53 INFO     [#updates[ 34200] train loss[4.09148] remaining[0:11:02]]
2018-11-22 10:01:22 INFO     [#updates[ 34300] train loss[4.08914] remaining[0:10:39]]
2018-11-22 10:01:49 INFO     [#updates[ 34400] train loss[4.08656] remaining[0:10:12]]
2018-11-22 10:02:13 INFO     [#updates[ 34500] train loss[4.08462] remaining[0:09:41]]
2018-11-22 10:02:43 INFO     [#updates[ 34600] train loss[4.08251] remaining[0:09:17]]
2018-11-22 10:03:11 INFO     [#updates[ 34700] train loss[4.08026] remaining[0:08:51]]
2018-11-22 10:03:37 INFO     [#updates[ 34800] train loss[4.07837] remaining[0:08:23]]
2018-11-22 10:04:05 INFO     [#updates[ 34900] train loss[4.07594] remaining[0:07:56]]
2018-11-22 10:04:33 INFO     [#updates[ 35000] train loss[4.07460] remaining[0:07:30]]
2018-11-22 10:05:00 INFO     [#updates[ 35100] train loss[4.07246] remaining[0:07:02]]
2018-11-22 10:05:27 INFO     [#updates[ 35200] train loss[4.06999] remaining[0:06:35]]
2018-11-22 10:05:53 INFO     [#updates[ 35300] train loss[4.06746] remaining[0:06:07]]
2018-11-22 10:06:18 INFO     [#updates[ 35400] train loss[4.06521] remaining[0:05:39]]
2018-11-22 10:06:43 INFO     [#updates[ 35500] train loss[4.06276] remaining[0:05:12]]
2018-11-22 10:07:10 INFO     [#updates[ 35600] train loss[4.06044] remaining[0:04:45]]
2018-11-22 10:07:36 INFO     [#updates[ 35700] train loss[4.05867] remaining[0:04:17]]
2018-11-22 10:08:03 INFO     [#updates[ 35800] train loss[4.05659] remaining[0:03:50]]
2018-11-22 10:08:29 INFO     [#updates[ 35900] train loss[4.05471] remaining[0:03:23]]
2018-11-22 10:08:55 INFO     [#updates[ 36000] train loss[4.05247] remaining[0:02:56]]
2018-11-22 10:09:21 INFO     [#updates[ 36100] train loss[4.05065] remaining[0:02:29]]
2018-11-22 10:09:50 INFO     [#updates[ 36200] train loss[4.04889] remaining[0:02:03]]
2018-11-22 10:10:16 INFO     [#updates[ 36300] train loss[4.04742] remaining[0:01:36]]
2018-11-22 10:10:40 INFO     [#updates[ 36400] train loss[4.04535] remaining[0:01:08]]
2018-11-22 10:11:07 INFO     [#updates[ 36500] train loss[4.04286] remaining[0:00:42]]
2018-11-22 10:11:32 INFO     [#updates[ 36600] train loss[4.04054] remaining[0:00:15]]
2018-11-22 10:12:33 INFO     [scheduler_type ms]
2018-11-22 10:12:38 INFO     [Saved the new best model and prediction]
2018-11-22 10:12:38 WARNING  [Epoch 8 - dev EM: 37.109 F1: 41.341 (best EM: 37.109 F1: 41.341)]
2018-11-22 10:12:38 WARNING  [Epoch 8 - ACC: 58.7804]
2018-11-22 10:12:38 WARNING  [Detailed Metric at Epoch 8: OrderedDict([('exact', 37.109407900277944), ('f1', 41.34114387908618), ('total', 11873), ('HasAns_exact', 74.32523616734143), ('HasAns_f1', 82.80084367010632), ('HasAns_total', 5928), ('NoAns_exact', 0.0), ('NoAns_f1', 0.0), ('NoAns_total', 5945)])]
2018-11-22 10:12:38 WARNING  [At epoch 9]
2018-11-22 10:12:38 INFO     [#updates[ 36658] train loss[4.03936] remaining[0:10:52]]
2018-11-22 10:12:46 INFO     [#updates[ 36700] train loss[4.03856] remaining[0:13:07]]
2018-11-22 10:13:05 INFO     [#updates[ 36800] train loss[4.03646] remaining[0:12:35]]
2018-11-22 10:13:34 INFO     [#updates[ 36900] train loss[4.03410] remaining[0:14:52]]
2018-11-22 10:14:03 INFO     [#updates[ 37000] train loss[4.03204] remaining[0:15:26]]
2018-11-22 10:14:33 INFO     [#updates[ 37100] train loss[4.02960] remaining[0:15:41]]
2018-11-22 10:15:01 INFO     [#updates[ 37200] train loss[4.02728] remaining[0:15:34]]
2018-11-22 10:15:30 INFO     [#updates[ 37300] train loss[4.02525] remaining[0:15:16]]
2018-11-22 10:15:57 INFO     [#updates[ 37400] train loss[4.02282] remaining[0:14:54]]
2018-11-22 10:16:25 INFO     [#updates[ 37500] train loss[4.02072] remaining[0:14:30]]
2018-11-22 10:16:52 INFO     [#updates[ 37600] train loss[4.01825] remaining[0:14:04]]
2018-11-22 10:17:20 INFO     [#updates[ 37700] train loss[4.01578] remaining[0:13:38]]
2018-11-22 10:17:47 INFO     [#updates[ 37800] train loss[4.01338] remaining[0:13:12]]
2018-11-22 10:18:15 INFO     [#updates[ 37900] train loss[4.01078] remaining[0:12:47]]
2018-11-22 10:18:36 INFO     [#updates[ 38000] train loss[4.00834] remaining[0:12:07]]
2018-11-22 10:19:03 INFO     [#updates[ 38100] train loss[4.00685] remaining[0:11:41]]
2018-11-22 10:19:29 INFO     [#updates[ 38200] train loss[4.00491] remaining[0:11:13]]
2018-11-22 10:19:56 INFO     [#updates[ 38300] train loss[4.00289] remaining[0:10:48]]
2018-11-22 10:20:23 INFO     [#updates[ 38400] train loss[4.00116] remaining[0:10:21]]
2018-11-22 10:20:56 INFO     [#updates[ 38500] train loss[3.99903] remaining[0:10:02]]
2018-11-22 10:21:24 INFO     [#updates[ 38600] train loss[3.99692] remaining[0:09:37]]
2018-11-22 10:21:54 INFO     [#updates[ 38700] train loss[3.99473] remaining[0:09:12]]
2018-11-22 10:22:23 INFO     [#updates[ 38800] train loss[3.99277] remaining[0:08:47]]
2018-11-22 10:22:53 INFO     [#updates[ 38900] train loss[3.99132] remaining[0:08:21]]
2018-11-22 10:23:20 INFO     [#updates[ 39000] train loss[3.98936] remaining[0:07:53]]
2018-11-22 10:23:47 INFO     [#updates[ 39100] train loss[3.98704] remaining[0:07:26]]
2018-11-22 10:24:15 INFO     [#updates[ 39200] train loss[3.98506] remaining[0:06:59]]
2018-11-22 10:24:46 INFO     [#updates[ 39300] train loss[3.98268] remaining[0:06:33]]
2018-11-22 10:25:12 INFO     [#updates[ 39400] train loss[3.98070] remaining[0:06:05]]
2018-11-22 10:25:41 INFO     [#updates[ 39500] train loss[3.97886] remaining[0:05:38]]
2018-11-22 10:26:09 INFO     [#updates[ 39600] train loss[3.97700] remaining[0:05:11]]
2018-11-22 10:26:36 INFO     [#updates[ 39700] train loss[3.97545] remaining[0:04:43]]
2018-11-22 10:27:04 INFO     [#updates[ 39800] train loss[3.97387] remaining[0:04:16]]
2018-11-22 10:27:32 INFO     [#updates[ 39900] train loss[3.97146] remaining[0:03:48]]
2018-11-22 10:27:59 INFO     [#updates[ 40000] train loss[3.96961] remaining[0:03:21]]
2018-11-22 10:28:26 INFO     [#updates[ 40100] train loss[3.96757] remaining[0:02:53]]
2018-11-22 10:28:56 INFO     [#updates[ 40200] train loss[3.96546] remaining[0:02:26]]
2018-11-22 10:29:26 INFO     [#updates[ 40300] train loss[3.96377] remaining[0:01:59]]
2018-11-22 10:29:56 INFO     [#updates[ 40400] train loss[3.96235] remaining[0:01:31]]
2018-11-22 10:30:25 INFO     [#updates[ 40500] train loss[3.96060] remaining[0:01:03]]
2018-11-22 10:30:53 INFO     [#updates[ 40600] train loss[3.95885] remaining[0:00:36]]
2018-11-22 10:31:22 INFO     [#updates[ 40700] train loss[3.95659] remaining[0:00:08]]
2018-11-22 10:32:18 INFO     [scheduler_type ms]
2018-11-22 10:32:23 INFO     [Saved the new best model and prediction]
2018-11-22 10:32:23 WARNING  [Epoch 9 - dev EM: 37.160 F1: 41.513 (best EM: 37.160 F1: 41.513)]
2018-11-22 10:32:23 WARNING  [Epoch 9 - ACC: 62.3094]
2018-11-22 10:32:23 WARNING  [Detailed Metric at Epoch 9: OrderedDict([('exact', 37.15994272719616), ('f1', 41.51304684762942), ('total', 11873), ('HasAns_exact', 74.42645074224022), ('HasAns_f1', 83.14514258129286), ('HasAns_total', 5928), ('NoAns_exact', 0.0), ('NoAns_f1', 0.0), ('NoAns_total', 5945)])]

You may want to consider changing your batch submission script as follows to speed up your job run next time:

#PBS -l select=1:ncpus=28:mem=6gb
#PBS -l place=free:shared
#PBS -l walltime=03:17:00

Your group nirav has been charged 03:12:51 for 28 cpus.
You previously had 4627:13:42.  You now have 4537:13:54 remaining for the queue oc_standard
