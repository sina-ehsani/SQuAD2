2018-11-22 05:16:08 INFO     [Launching the SAN]
2018-11-22 05:16:08 INFO     [Loading data]
Loaded 130319 samples out of 130319
Loaded 11873 samples out of 11873
2018-11-22 05:16:37 INFO     [
############# Model Arch of SAN #############
DNetwork(
  (dropout): DropoutWrapper()
  (lexicon_encoder): LexiconEncoder(
    (dropout): DropoutWrapper()
    (dropout_emb): DropoutWrapper()
    (dropout_cove): DropoutWrapper()
    (embedding): Embedding(90953, 300, padding_idx=0)
    (ContextualEmbed): ContextualEmbed(
      (embedding): Embedding(90953, 300, padding_idx=0)
      (rnn1): LSTM(300, 300, bidirectional=True)
      (rnn2): LSTM(600, 300, bidirectional=True)
    )
    (prealign): AttentionWrapper(
      (score_func): SimilarityWrapper(
        (score_func): DotProductProject(
          (dropout): DropoutWrapper()
          (proj_1): Linear(in_features=300, out_features=128, bias=False)
          (proj_2): Linear(in_features=300, out_features=128, bias=False)
        )
      )
    )
    (pos_embedding): Embedding(54, 12, padding_idx=0)
    (ner_embedding): Embedding(41, 8, padding_idx=0)
    (doc_pwnn): PositionwiseNN(
      (w_0): Conv1d(1224, 256, kernel_size=(1,), stride=(1,))
      (w_1): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
      (dropout): DropoutWrapper()
    )
    (que_pwnn): PositionwiseNN(
      (w_0): Conv1d(900, 256, kernel_size=(1,), stride=(1,))
      (w_1): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
      (dropout): DropoutWrapper()
    )
  )
  (doc_encoder_low): OneLayerBRNN(
    (dropout): DropoutWrapper()
    (rnn): LSTM(856, 128, bidirectional=True)
  )
  (doc_encoder_high): OneLayerBRNN(
    (dropout): DropoutWrapper()
    (rnn): LSTM(856, 128, bidirectional=True)
  )
  (query_encoder_low): OneLayerBRNN(
    (dropout): DropoutWrapper()
    (rnn): LSTM(856, 128, bidirectional=True)
  )
  (query_encoder_high): OneLayerBRNN(
    (dropout): DropoutWrapper()
    (rnn): LSTM(856, 128, bidirectional=True)
  )
  (query_understand): OneLayerBRNN(
    (dropout): DropoutWrapper()
    (rnn): LSTM(512, 128, bidirectional=True)
  )
  (deep_attn): DeepAttentionWrapper(
    (dropout): DropoutWrapper()
    (attn_list): ModuleList(
      (0): AttentionWrapper(
        (score_func): SimilarityWrapper(
          (score_func): DotProductProject(
            (dropout): DropoutWrapper()
            (proj_1): Linear(in_features=1412, out_features=128, bias=False)
            (proj_2): Linear(in_features=1412, out_features=128, bias=False)
          )
        )
      )
      (1): AttentionWrapper(
        (score_func): SimilarityWrapper(
          (score_func): DotProductProject(
            (dropout): DropoutWrapper()
            (proj_1): Linear(in_features=1412, out_features=128, bias=False)
            (proj_2): Linear(in_features=1412, out_features=128, bias=False)
          )
        )
      )
      (2): AttentionWrapper(
        (score_func): SimilarityWrapper(
          (score_func): DotProductProject(
            (dropout): DropoutWrapper()
            (proj_1): Linear(in_features=1412, out_features=128, bias=False)
            (proj_2): Linear(in_features=1412, out_features=128, bias=False)
          )
        )
      )
    )
  )
  (doc_understand): OneLayerBRNN(
    (dropout): DropoutWrapper()
    (rnn): LSTM(1280, 128, bidirectional=True)
  )
  (doc_self_attn): AttentionWrapper(
    (score_func): SimilarityWrapper(
      (score_func): DotProductProject(
        (dropout): DropoutWrapper()
        (proj_1): Linear(in_features=2436, out_features=128, bias=False)
        (proj_2): Linear(in_features=2436, out_features=128, bias=False)
      )
    )
  )
  (doc_mem_gen): OneLayerBRNN(
    (dropout): DropoutWrapper()
    (rnn): LSTM(512, 128, bidirectional=True)
  )
  (query_sum_attn): SelfAttnWrapper(
    (att): LinearSelfAttn(
      (linear): Linear(in_features=256, out_features=1, bias=True)
      (dropout): DropoutWrapper()
    )
  )
  (decoder): SAN(
    (attn_b): FlatSimilarityWrapper(
      (att_dropout): DropoutWrapper()
      (score_func): BilinearFlatSim(
        (linear): Linear(in_features=256, out_features=256, bias=True)
        (dropout): DropoutWrapper()
      )
    )
    (attn_e): FlatSimilarityWrapper(
      (att_dropout): DropoutWrapper()
      (score_func): BilinearFlatSim(
        (linear): Linear(in_features=256, out_features=256, bias=True)
        (dropout): DropoutWrapper()
      )
    )
    (rnn): GRUCell(256, 256)
    (dropout): DropoutWrapper()
  )
  (doc_sum_attn): SelfAttnWrapper(
    (att): LinearSelfAttn(
      (linear): Linear(in_features=256, out_features=1, bias=True)
      (dropout): DropoutWrapper()
    )
  )
  (classifier): Classifier(
    (dropout): DropoutWrapper()
    (proj): Linear(in_features=512, out_features=1, bias=True)
  )
)
]
2018-11-22 05:16:38 INFO     [Total number of params: 9194805]
2018-11-22 05:16:43 WARNING  [At epoch 0]
2018-11-22 05:16:43 INFO     [#updates[     1] train loss[10.56736] remaining[0:16:31]]
2018-11-22 05:17:06 INFO     [#updates[   100] train loss[9.48431] remaining[0:15:15]]
2018-11-22 05:17:30 INFO     [#updates[   200] train loss[9.02582] remaining[0:15:07]]
2018-11-22 05:17:53 INFO     [#updates[   300] train loss[8.70641] remaining[0:14:40]]
2018-11-22 05:18:17 INFO     [#updates[   400] train loss[8.48439] remaining[0:14:22]]
2018-11-22 05:18:42 INFO     [#updates[   500] train loss[8.32754] remaining[0:14:06]]
2018-11-22 05:19:05 INFO     [#updates[   600] train loss[8.17711] remaining[0:13:40]]
2018-11-22 05:19:28 INFO     [#updates[   700] train loss[8.10012] remaining[0:13:15]]
2018-11-22 05:19:53 INFO     [#updates[   800] train loss[7.99819] remaining[0:12:55]]
2018-11-22 05:20:16 INFO     [#updates[   900] train loss[7.91295] remaining[0:12:29]]
2018-11-22 05:20:40 INFO     [#updates[  1000] train loss[7.83065] remaining[0:12:06]]
2018-11-22 05:21:03 INFO     [#updates[  1100] train loss[7.77066] remaining[0:11:43]]
2018-11-22 05:21:27 INFO     [#updates[  1200] train loss[7.70510] remaining[0:11:19]]
2018-11-22 05:21:50 INFO     [#updates[  1300] train loss[7.65807] remaining[0:10:55]]
2018-11-22 05:22:13 INFO     [#updates[  1400] train loss[7.58483] remaining[0:10:29]]
2018-11-22 05:22:37 INFO     [#updates[  1500] train loss[7.53340] remaining[0:10:06]]
2018-11-22 05:22:59 INFO     [#updates[  1600] train loss[7.48613] remaining[0:09:41]]
2018-11-22 05:23:23 INFO     [#updates[  1700] train loss[7.43256] remaining[0:09:18]]
2018-11-22 05:23:47 INFO     [#updates[  1800] train loss[7.38827] remaining[0:08:55]]
2018-11-22 05:24:11 INFO     [#updates[  1900] train loss[7.33260] remaining[0:08:31]]
2018-11-22 05:24:34 INFO     [#updates[  2000] train loss[7.28183] remaining[0:08:07]]
2018-11-22 05:24:57 INFO     [#updates[  2100] train loss[7.22518] remaining[0:07:44]]
2018-11-22 05:25:21 INFO     [#updates[  2200] train loss[7.16981] remaining[0:07:20]]
2018-11-22 05:25:44 INFO     [#updates[  2300] train loss[7.12017] remaining[0:06:57]]
2018-11-22 05:26:08 INFO     [#updates[  2400] train loss[7.07056] remaining[0:06:33]]
2018-11-22 05:26:31 INFO     [#updates[  2500] train loss[7.02821] remaining[0:06:09]]
2018-11-22 05:26:54 INFO     [#updates[  2600] train loss[6.97662] remaining[0:05:46]]
2018-11-22 05:27:18 INFO     [#updates[  2700] train loss[6.92889] remaining[0:05:22]]
2018-11-22 05:27:42 INFO     [#updates[  2800] train loss[6.88628] remaining[0:04:59]]
2018-11-22 05:28:06 INFO     [#updates[  2900] train loss[6.84295] remaining[0:04:36]]
2018-11-22 05:28:31 INFO     [#updates[  3000] train loss[6.79200] remaining[0:04:13]]
2018-11-22 05:28:54 INFO     [#updates[  3100] train loss[6.74641] remaining[0:03:49]]
2018-11-22 05:29:18 INFO     [#updates[  3200] train loss[6.70188] remaining[0:03:25]]
2018-11-22 05:29:42 INFO     [#updates[  3300] train loss[6.65305] remaining[0:03:02]]
2018-11-22 05:30:05 INFO     [#updates[  3400] train loss[6.61123] remaining[0:02:38]]
2018-11-22 05:30:29 INFO     [#updates[  3500] train loss[6.57315] remaining[0:02:15]]
2018-11-22 05:30:53 INFO     [#updates[  3600] train loss[6.53307] remaining[0:01:51]]
2018-11-22 05:31:18 INFO     [#updates[  3700] train loss[6.49618] remaining[0:01:28]]
2018-11-22 05:31:41 INFO     [#updates[  3800] train loss[6.45688] remaining[0:01:04]]
2018-11-22 05:32:05 INFO     [#updates[  3900] train loss[6.42179] remaining[0:00:40]]
2018-11-22 05:32:29 INFO     [#updates[  4000] train loss[6.38617] remaining[0:00:17]]
2018-11-22 05:33:30 INFO     [scheduler_type ms]
2018-11-22 05:33:45 INFO     [Saved the new best model and prediction]
2018-11-22 05:33:45 WARNING  [Epoch 0 - dev EM: 28.308 F1: 34.154 (best EM: 28.308 F1: 34.154)]
2018-11-22 05:33:45 WARNING  [Epoch 0 - ACC: 50.7285]
2018-11-22 05:33:45 WARNING  [Detailed Metric at Epoch 0: OrderedDict([('exact', 28.30792554535501), ('f1', 34.15402487259596), ('total', 11873), ('HasAns_exact', 56.68016194331984), ('HasAns_f1', 68.3891257274514), ('HasAns_total', 5928), ('NoAns_exact', 0.01682085786375105), ('NoAns_f1', 0.01682085786375105), ('NoAns_total', 5945)])]
2018-11-22 05:33:45 WARNING  [At epoch 1]
2018-11-22 05:33:45 INFO     [#updates[  4074] train loss[6.35939] remaining[0:11:43]]
2018-11-22 05:33:51 INFO     [#updates[  4100] train loss[6.35079] remaining[0:14:03]]
2018-11-22 05:34:12 INFO     [#updates[  4200] train loss[6.30998] remaining[0:13:53]]
2018-11-22 05:34:36 INFO     [#updates[  4300] train loss[6.27709] remaining[0:14:12]]
2018-11-22 05:34:59 INFO     [#updates[  4400] train loss[6.24346] remaining[0:14:01]]
2018-11-22 05:35:23 INFO     [#updates[  4500] train loss[6.21246] remaining[0:13:58]]
2018-11-22 05:35:42 INFO     [#updates[  4600] train loss[6.18373] remaining[0:13:05]]
2018-11-22 05:36:05 INFO     [#updates[  4700] train loss[6.15394] remaining[0:12:48]]
2018-11-22 05:36:29 INFO     [#updates[  4800] train loss[6.12467] remaining[0:12:32]]
2018-11-22 05:36:52 INFO     [#updates[  4900] train loss[6.09622] remaining[0:12:12]]
2018-11-22 05:37:15 INFO     [#updates[  5000] train loss[6.06818] remaining[0:11:53]]
2018-11-22 05:37:40 INFO     [#updates[  5100] train loss[6.04401] remaining[0:11:36]]
2018-11-22 05:38:04 INFO     [#updates[  5200] train loss[6.01423] remaining[0:11:15]]
2018-11-22 05:38:28 INFO     [#updates[  5300] train loss[5.99248] remaining[0:10:54]]
2018-11-22 05:38:51 INFO     [#updates[  5400] train loss[5.96877] remaining[0:10:33]]
2018-11-22 05:39:15 INFO     [#updates[  5500] train loss[5.94723] remaining[0:10:11]]
2018-11-22 05:39:40 INFO     [#updates[  5600] train loss[5.92461] remaining[0:09:50]]
2018-11-22 05:40:03 INFO     [#updates[  5700] train loss[5.90348] remaining[0:09:28]]
2018-11-22 05:40:28 INFO     [#updates[  5800] train loss[5.88116] remaining[0:09:07]]
2018-11-22 05:40:53 INFO     [#updates[  5900] train loss[5.85776] remaining[0:08:45]]
2018-11-22 05:41:17 INFO     [#updates[  6000] train loss[5.83837] remaining[0:08:22]]
2018-11-22 05:41:40 INFO     [#updates[  6100] train loss[5.81799] remaining[0:07:59]]
2018-11-22 05:42:04 INFO     [#updates[  6200] train loss[5.79649] remaining[0:07:36]]
2018-11-22 05:42:24 INFO     [#updates[  6300] train loss[5.77592] remaining[0:07:09]]
2018-11-22 05:42:47 INFO     [#updates[  6400] train loss[5.75956] remaining[0:06:46]]
2018-11-22 05:43:10 INFO     [#updates[  6500] train loss[5.74109] remaining[0:06:22]]
2018-11-22 05:43:33 INFO     [#updates[  6600] train loss[5.72366] remaining[0:05:59]]
2018-11-22 05:43:57 INFO     [#updates[  6700] train loss[5.70109] remaining[0:05:36]]
2018-11-22 05:44:20 INFO     [#updates[  6800] train loss[5.68430] remaining[0:05:13]]
2018-11-22 05:44:43 INFO     [#updates[  6900] train loss[5.66630] remaining[0:04:50]]
2018-11-22 05:45:07 INFO     [#updates[  7000] train loss[5.64830] remaining[0:04:27]]
2018-11-22 05:45:27 INFO     [#updates[  7100] train loss[5.63014] remaining[0:04:02]]
2018-11-22 05:45:52 INFO     [#updates[  7200] train loss[5.61485] remaining[0:03:39]]
2018-11-22 05:46:17 INFO     [#updates[  7300] train loss[5.60047] remaining[0:03:16]]
2018-11-22 05:46:41 INFO     [#updates[  7400] train loss[5.58471] remaining[0:02:53]]
2018-11-22 05:47:05 INFO     [#updates[  7500] train loss[5.56707] remaining[0:02:30]]
2018-11-22 05:47:25 INFO     [#updates[  7600] train loss[5.55194] remaining[0:02:06]]
2018-11-22 05:47:48 INFO     [#updates[  7700] train loss[5.53828] remaining[0:01:43]]
2018-11-22 05:48:13 INFO     [#updates[  7800] train loss[5.52347] remaining[0:01:20]]
2018-11-22 05:48:37 INFO     [#updates[  7900] train loss[5.50774] remaining[0:00:57]]
2018-11-22 05:49:03 INFO     [#updates[  8000] train loss[5.49628] remaining[0:00:34]]
2018-11-22 05:49:27 INFO     [#updates[  8100] train loss[5.48176] remaining[0:00:10]]
2018-11-22 05:50:22 INFO     [scheduler_type ms]
2018-11-22 05:50:37 INFO     [Saved the new best model and prediction]
2018-11-22 05:50:37 WARNING  [Epoch 1 - dev EM: 32.848 F1: 37.646 (best EM: 32.848 F1: 37.646)]
2018-11-22 05:50:37 WARNING  [Epoch 1 - ACC: 55.5883]
2018-11-22 05:50:37 WARNING  [Detailed Metric at Epoch 1: OrderedDict([('exact', 32.84763749684157), ('f1', 37.64593335437168), ('total', 11873), ('HasAns_exact', 65.77260458839406), ('HasAns_f1', 75.38295659859226), ('HasAns_total', 5928), ('NoAns_exact', 0.01682085786375105), ('NoAns_f1', 0.01682085786375105), ('NoAns_total', 5945)])]
2018-11-22 05:50:37 WARNING  [At epoch 2]
2018-11-22 05:50:37 INFO     [#updates[  8147] train loss[5.47500] remaining[0:10:17]]
2018-11-22 05:50:50 INFO     [#updates[  8200] train loss[5.46876] remaining[0:15:41]]
2018-11-22 05:51:15 INFO     [#updates[  8300] train loss[5.45397] remaining[0:15:46]]
2018-11-22 05:51:38 INFO     [#updates[  8400] train loss[5.43892] remaining[0:15:16]]
2018-11-22 05:52:02 INFO     [#updates[  8500] train loss[5.42356] remaining[0:14:46]]
2018-11-22 05:52:26 INFO     [#updates[  8600] train loss[5.41123] remaining[0:14:26]]
2018-11-22 05:52:49 INFO     [#updates[  8700] train loss[5.39592] remaining[0:13:57]]
2018-11-22 05:53:13 INFO     [#updates[  8800] train loss[5.38379] remaining[0:13:34]]
2018-11-22 05:53:38 INFO     [#updates[  8900] train loss[5.37052] remaining[0:13:14]]
2018-11-22 05:54:02 INFO     [#updates[  9000] train loss[5.35752] remaining[0:12:52]]
2018-11-22 05:54:27 INFO     [#updates[  9100] train loss[5.34430] remaining[0:12:31]]
2018-11-22 05:54:52 INFO     [#updates[  9200] train loss[5.33183] remaining[0:12:08]]
2018-11-22 05:55:16 INFO     [#updates[  9300] train loss[5.31838] remaining[0:11:43]]
2018-11-22 05:55:41 INFO     [#updates[  9400] train loss[5.30526] remaining[0:11:22]]
2018-11-22 05:56:05 INFO     [#updates[  9500] train loss[5.29212] remaining[0:10:58]]
2018-11-22 05:56:30 INFO     [#updates[  9600] train loss[5.28103] remaining[0:10:35]]
2018-11-22 05:56:56 INFO     [#updates[  9700] train loss[5.26925] remaining[0:10:13]]
2018-11-22 05:57:21 INFO     [#updates[  9800] train loss[5.25685] remaining[0:09:49]]
2018-11-22 05:57:46 INFO     [#updates[  9900] train loss[5.24394] remaining[0:09:26]]
2018-11-22 05:58:09 INFO     [#updates[ 10000] train loss[5.23428] remaining[0:09:01]]
2018-11-22 05:58:33 INFO     [#updates[ 10100] train loss[5.22502] remaining[0:08:35]]
2018-11-22 05:58:58 INFO     [#updates[ 10200] train loss[5.21493] remaining[0:08:11]]
2018-11-22 05:59:22 INFO     [#updates[ 10300] train loss[5.20366] remaining[0:07:47]]
2018-11-22 05:59:46 INFO     [#updates[ 10400] train loss[5.19331] remaining[0:07:22]]
2018-11-22 06:00:09 INFO     [#updates[ 10500] train loss[5.18331] remaining[0:06:57]]
2018-11-22 06:00:34 INFO     [#updates[ 10600] train loss[5.17260] remaining[0:06:33]]
2018-11-22 06:00:58 INFO     [#updates[ 10700] train loss[5.16187] remaining[0:06:09]]
2018-11-22 06:01:22 INFO     [#updates[ 10800] train loss[5.15259] remaining[0:05:44]]
2018-11-22 06:01:48 INFO     [#updates[ 10900] train loss[5.14316] remaining[0:05:21]]
2018-11-22 06:02:12 INFO     [#updates[ 11000] train loss[5.13240] remaining[0:04:56]]
2018-11-22 06:02:36 INFO     [#updates[ 11100] train loss[5.12170] remaining[0:04:32]]
2018-11-22 06:03:00 INFO     [#updates[ 11200] train loss[5.11230] remaining[0:04:07]]
2018-11-22 06:03:24 INFO     [#updates[ 11300] train loss[5.10260] remaining[0:03:43]]
2018-11-22 06:03:48 INFO     [#updates[ 11400] train loss[5.09380] remaining[0:03:19]]
2018-11-22 06:04:12 INFO     [#updates[ 11500] train loss[5.08505] remaining[0:02:54]]
2018-11-22 06:04:37 INFO     [#updates[ 11600] train loss[5.07604] remaining[0:02:30]]
2018-11-22 06:05:01 INFO     [#updates[ 11700] train loss[5.06808] remaining[0:02:06]]
2018-11-22 06:05:24 INFO     [#updates[ 11800] train loss[5.06190] remaining[0:01:41]]
2018-11-22 06:05:48 INFO     [#updates[ 11900] train loss[5.05466] remaining[0:01:17]]
2018-11-22 06:06:14 INFO     [#updates[ 12000] train loss[5.04478] remaining[0:00:53]]
2018-11-22 06:06:38 INFO     [#updates[ 12100] train loss[5.03676] remaining[0:00:28]]
2018-11-22 06:07:02 INFO     [#updates[ 12200] train loss[5.02777] remaining[0:00:04]]
2018-11-22 06:07:51 INFO     [scheduler_type ms]
2018-11-22 06:08:04 INFO     [Saved the new best model and prediction]
2018-11-22 06:08:04 WARNING  [Epoch 2 - dev EM: 33.841 F1: 39.035 (best EM: 33.841 F1: 39.035)]
2018-11-22 06:08:04 WARNING  [Epoch 2 - ACC: 56.3295]
2018-11-22 06:08:04 WARNING  [Detailed Metric at Epoch 2: OrderedDict([('exact', 33.841489092899856), ('f1', 39.03503913023765), ('total', 11873), ('HasAns_exact', 67.78002699055331), ('HasAns_f1', 78.18202084907416), ('HasAns_total', 5928), ('NoAns_exact', 0.0), ('NoAns_f1', 0.0), ('NoAns_total', 5945)])]
2018-11-22 06:08:04 WARNING  [At epoch 3]
2018-11-22 06:08:04 INFO     [#updates[ 12220] train loss[5.02607] remaining[0:10:33]]
2018-11-22 06:08:24 INFO     [#updates[ 12300] train loss[5.01957] remaining[0:16:01]]
2018-11-22 06:08:48 INFO     [#updates[ 12400] train loss[5.01223] remaining[0:15:40]]
2018-11-22 06:09:11 INFO     [#updates[ 12500] train loss[5.00293] remaining[0:15:08]]
2018-11-22 06:09:34 INFO     [#updates[ 12600] train loss[4.99362] remaining[0:14:30]]
2018-11-22 06:09:57 INFO     [#updates[ 12700] train loss[4.98497] remaining[0:14:02]]
2018-11-22 06:10:21 INFO     [#updates[ 12800] train loss[4.97580] remaining[0:13:40]]
2018-11-22 06:10:45 INFO     [#updates[ 12900] train loss[4.96696] remaining[0:13:22]]
2018-11-22 06:11:10 INFO     [#updates[ 13000] train loss[4.95769] remaining[0:13:04]]
2018-11-22 06:11:34 INFO     [#updates[ 13100] train loss[4.95031] remaining[0:12:39]]
2018-11-22 06:11:58 INFO     [#updates[ 13200] train loss[4.94354] remaining[0:12:16]]
2018-11-22 06:12:22 INFO     [#updates[ 13300] train loss[4.93581] remaining[0:11:53]]
2018-11-22 06:12:47 INFO     [#updates[ 13400] train loss[4.92767] remaining[0:11:32]]
2018-11-22 06:13:11 INFO     [#updates[ 13500] train loss[4.92102] remaining[0:11:08]]
2018-11-22 06:13:36 INFO     [#updates[ 13600] train loss[4.91265] remaining[0:10:46]]
2018-11-22 06:14:01 INFO     [#updates[ 13700] train loss[4.90474] remaining[0:10:23]]
2018-11-22 06:14:25 INFO     [#updates[ 13800] train loss[4.89722] remaining[0:10:01]]
2018-11-22 06:14:50 INFO     [#updates[ 13900] train loss[4.88925] remaining[0:09:37]]
2018-11-22 06:15:15 INFO     [#updates[ 14000] train loss[4.88327] remaining[0:09:14]]
2018-11-22 06:15:39 INFO     [#updates[ 14100] train loss[4.87566] remaining[0:08:50]]
2018-11-22 06:16:03 INFO     [#updates[ 14200] train loss[4.86875] remaining[0:08:25]]
2018-11-22 06:16:28 INFO     [#updates[ 14300] train loss[4.86139] remaining[0:08:02]]
2018-11-22 06:16:52 INFO     [#updates[ 14400] train loss[4.85500] remaining[0:07:37]]
2018-11-22 06:17:17 INFO     [#updates[ 14500] train loss[4.84644] remaining[0:07:14]]
2018-11-22 06:17:41 INFO     [#updates[ 14600] train loss[4.84252] remaining[0:06:49]]
2018-11-22 06:18:05 INFO     [#updates[ 14700] train loss[4.83577] remaining[0:06:25]]
2018-11-22 06:18:31 INFO     [#updates[ 14800] train loss[4.82911] remaining[0:06:02]]
2018-11-22 06:18:56 INFO     [#updates[ 14900] train loss[4.82224] remaining[0:05:38]]
2018-11-22 06:19:22 INFO     [#updates[ 15000] train loss[4.81580] remaining[0:05:14]]
2018-11-22 06:19:47 INFO     [#updates[ 15100] train loss[4.80936] remaining[0:04:50]]
2018-11-22 06:20:13 INFO     [#updates[ 15200] train loss[4.80308] remaining[0:04:26]]
2018-11-22 06:20:38 INFO     [#updates[ 15300] train loss[4.79573] remaining[0:04:02]]
2018-11-22 06:21:04 INFO     [#updates[ 15400] train loss[4.78983] remaining[0:03:38]]
2018-11-22 06:21:29 INFO     [#updates[ 15500] train loss[4.78408] remaining[0:03:14]]
2018-11-22 06:21:55 INFO     [#updates[ 15600] train loss[4.77737] remaining[0:02:50]]
2018-11-22 06:22:19 INFO     [#updates[ 15700] train loss[4.77124] remaining[0:02:25]]
2018-11-22 06:22:45 INFO     [#updates[ 15800] train loss[4.76520] remaining[0:02:01]]
2018-11-22 06:23:10 INFO     [#updates[ 15900] train loss[4.76023] remaining[0:01:36]]
2018-11-22 06:23:34 INFO     [#updates[ 16000] train loss[4.75326] remaining[0:01:11]]
2018-11-22 06:23:59 INFO     [#updates[ 16100] train loss[4.74829] remaining[0:00:47]]
2018-11-22 06:24:23 INFO     [#updates[ 16200] train loss[4.74332] remaining[0:00:22]]
2018-11-22 06:25:31 INFO     [scheduler_type ms]
2018-11-22 06:25:45 INFO     [Saved the new best model and prediction]
2018-11-22 06:25:45 WARNING  [Epoch 3 - dev EM: 35.374 F1: 39.794 (best EM: 35.374 F1: 39.794)]
2018-11-22 06:25:45 WARNING  [Epoch 3 - ACC: 61.0292]
2018-11-22 06:25:45 WARNING  [Detailed Metric at Epoch 3: OrderedDict([('exact', 35.37437884275246), ('f1', 39.79385008147179), ('total', 11873), ('HasAns_exact', 70.8502024291498), ('HasAns_f1', 79.70181882883173), ('HasAns_total', 5928), ('NoAns_exact', 0.0), ('NoAns_f1', 0.0), ('NoAns_total', 5945)])]
2018-11-22 06:25:45 WARNING  [At epoch 4]
2018-11-22 06:25:45 INFO     [#updates[ 16293] train loss[4.73914] remaining[0:15:40]]
2018-11-22 06:25:47 INFO     [#updates[ 16300] train loss[4.73866] remaining[0:15:52]]
2018-11-22 06:26:11 INFO     [#updates[ 16400] train loss[4.73226] remaining[0:16:14]]
2018-11-22 06:26:34 INFO     [#updates[ 16500] train loss[4.72666] remaining[0:15:19]]
2018-11-22 06:26:57 INFO     [#updates[ 16600] train loss[4.72236] remaining[0:14:47]]
2018-11-22 06:27:20 INFO     [#updates[ 16700] train loss[4.71520] remaining[0:14:19]]
2018-11-22 06:27:45 INFO     [#updates[ 16800] train loss[4.70964] remaining[0:14:06]]
2018-11-22 06:28:10 INFO     [#updates[ 16900] train loss[4.70281] remaining[0:13:45]]
2018-11-22 06:28:35 INFO     [#updates[ 17000] train loss[4.69851] remaining[0:13:29]]
2018-11-22 06:29:00 INFO     [#updates[ 17100] train loss[4.69370] remaining[0:13:08]]
2018-11-22 06:29:23 INFO     [#updates[ 17200] train loss[4.68832] remaining[0:12:42]]
2018-11-22 06:29:48 INFO     [#updates[ 17300] train loss[4.68098] remaining[0:12:20]]
2018-11-22 06:30:13 INFO     [#updates[ 17400] train loss[4.67560] remaining[0:11:56]]
2018-11-22 06:30:37 INFO     [#updates[ 17500] train loss[4.66839] remaining[0:11:32]]
2018-11-22 06:31:02 INFO     [#updates[ 17600] train loss[4.66260] remaining[0:11:09]]
2018-11-22 06:31:26 INFO     [#updates[ 17700] train loss[4.65763] remaining[0:10:45]]
2018-11-22 06:31:49 INFO     [#updates[ 17800] train loss[4.65200] remaining[0:10:19]]
2018-11-22 06:32:13 INFO     [#updates[ 17900] train loss[4.64642] remaining[0:09:55]]
2018-11-22 06:32:39 INFO     [#updates[ 18000] train loss[4.64022] remaining[0:09:33]]
2018-11-22 06:33:04 INFO     [#updates[ 18100] train loss[4.63425] remaining[0:09:09]]
2018-11-22 06:33:29 INFO     [#updates[ 18200] train loss[4.62938] remaining[0:08:46]]
2018-11-22 06:33:52 INFO     [#updates[ 18300] train loss[4.62546] remaining[0:08:20]]
2018-11-22 06:34:16 INFO     [#updates[ 18400] train loss[4.62081] remaining[0:07:56]]
2018-11-22 06:34:40 INFO     [#updates[ 18500] train loss[4.61617] remaining[0:07:31]]
2018-11-22 06:35:03 INFO     [#updates[ 18600] train loss[4.61108] remaining[0:07:06]]
2018-11-22 06:35:27 INFO     [#updates[ 18700] train loss[4.60672] remaining[0:06:42]]
2018-11-22 06:35:52 INFO     [#updates[ 18800] train loss[4.60132] remaining[0:06:18]]
2018-11-22 06:36:16 INFO     [#updates[ 18900] train loss[4.59682] remaining[0:05:54]]
2018-11-22 06:36:40 INFO     [#updates[ 19000] train loss[4.59217] remaining[0:05:30]]
2018-11-22 06:37:04 INFO     [#updates[ 19100] train loss[4.58802] remaining[0:05:05]]
2018-11-22 06:37:29 INFO     [#updates[ 19200] train loss[4.58365] remaining[0:04:41]]
2018-11-22 06:37:53 INFO     [#updates[ 19300] train loss[4.57846] remaining[0:04:17]]
2018-11-22 06:38:18 INFO     [#updates[ 19400] train loss[4.57504] remaining[0:03:53]]
2018-11-22 06:38:42 INFO     [#updates[ 19500] train loss[4.57094] remaining[0:03:29]]
2018-11-22 06:39:06 INFO     [#updates[ 19600] train loss[4.56598] remaining[0:03:05]]
2018-11-22 06:39:30 INFO     [#updates[ 19700] train loss[4.56118] remaining[0:02:41]]
2018-11-22 06:39:55 INFO     [#updates[ 19800] train loss[4.55689] remaining[0:02:16]]
2018-11-22 06:40:19 INFO     [#updates[ 19900] train loss[4.55305] remaining[0:01:52]]
2018-11-22 06:40:44 INFO     [#updates[ 20000] train loss[4.54981] remaining[0:01:28]]
2018-11-22 06:41:08 INFO     [#updates[ 20100] train loss[4.54502] remaining[0:01:04]]
2018-11-22 06:41:33 INFO     [#updates[ 20200] train loss[4.53982] remaining[0:00:40]]
2018-11-22 06:41:58 INFO     [#updates[ 20300] train loss[4.53573] remaining[0:00:15]]
2018-11-22 06:43:00 INFO     [scheduler_type ms]
2018-11-22 06:43:13 INFO     [Saved the new best model and prediction]
2018-11-22 06:43:13 WARNING  [Epoch 4 - dev EM: 35.829 F1: 40.267 (best EM: 35.829 F1: 40.267)]
2018-11-22 06:43:13 WARNING  [Epoch 4 - ACC: 61.4588]
2018-11-22 06:43:13 WARNING  [Detailed Metric at Epoch 4: OrderedDict([('exact', 35.829192285016426), ('f1', 40.267303360332114), ('total', 11873), ('HasAns_exact', 71.76113360323886), ('HasAns_f1', 80.65008313043576), ('HasAns_total', 5928), ('NoAns_exact', 0.0), ('NoAns_f1', 0.0), ('NoAns_total', 5945)])]

You may want to consider changing your batch submission script as follows to speed up your job run next time:

#PBS -l select=1:ncpus=28:mem=6gb
#PBS -l place=free:shared
#PBS -l walltime=01:32:00

Your group nirav has been charged 01:27:13 for 28 cpus.
You previously had 4667:55:46.  You now have 4627:13:42 remaining for the queue oc_standard
