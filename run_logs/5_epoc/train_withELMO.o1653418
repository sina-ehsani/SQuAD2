2018-11-21 07:39:57 INFO     [Launching the SAN]
2018-11-21 07:39:57 INFO     [Loading data]
Loaded 130319 samples out of 130319
Loaded 11873 samples out of 11873
2018-11-21 07:40:25 INFO     [
############# Model Arch of SAN #############
DNetwork(
  (dropout): DropoutWrapper()
  (lexicon_encoder): LexiconEncoder(
    (dropout): DropoutWrapper()
    (dropout_emb): DropoutWrapper()
    (dropout_cove): DropoutWrapper()
    (embedding): Embedding(90953, 300, padding_idx=0)
    (ContextualEmbed): ContextualEmbed(
      (embedding): Embedding(90953, 300, padding_idx=0)
      (rnn1): LSTM(300, 300, bidirectional=True)
      (rnn2): LSTM(600, 300, bidirectional=True)
    )
    (prealign): AttentionWrapper(
      (score_func): SimilarityWrapper(
        (score_func): DotProductProject(
          (dropout): DropoutWrapper()
          (proj_1): Linear(in_features=300, out_features=128, bias=False)
          (proj_2): Linear(in_features=300, out_features=128, bias=False)
        )
      )
    )
    (pos_embedding): Embedding(54, 12, padding_idx=0)
    (ner_embedding): Embedding(41, 8, padding_idx=0)
    (doc_pwnn): PositionwiseNN(
      (w_0): Conv1d(1224, 256, kernel_size=(1,), stride=(1,))
      (w_1): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
      (dropout): DropoutWrapper()
    )
    (que_pwnn): PositionwiseNN(
      (w_0): Conv1d(900, 256, kernel_size=(1,), stride=(1,))
      (w_1): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
      (dropout): DropoutWrapper()
    )
  )
  (doc_encoder_low): OneLayerBRNN(
    (dropout): DropoutWrapper()
    (rnn): LSTM(856, 128, bidirectional=True)
  )
  (doc_encoder_high): OneLayerBRNN(
    (dropout): DropoutWrapper()
    (rnn): LSTM(856, 128, bidirectional=True)
  )
  (query_encoder_low): OneLayerBRNN(
    (dropout): DropoutWrapper()
    (rnn): LSTM(856, 128, bidirectional=True)
  )
  (query_encoder_high): OneLayerBRNN(
    (dropout): DropoutWrapper()
    (rnn): LSTM(856, 128, bidirectional=True)
  )
  (query_understand): OneLayerBRNN(
    (dropout): DropoutWrapper()
    (rnn): LSTM(512, 128, bidirectional=True)
  )
  (deep_attn): DeepAttentionWrapper(
    (dropout): DropoutWrapper()
    (attn_list): ModuleList(
      (0): AttentionWrapper(
        (score_func): SimilarityWrapper(
          (score_func): DotProductProject(
            (dropout): DropoutWrapper()
            (proj_1): Linear(in_features=1412, out_features=128, bias=False)
            (proj_2): Linear(in_features=1412, out_features=128, bias=False)
          )
        )
      )
      (1): AttentionWrapper(
        (score_func): SimilarityWrapper(
          (score_func): DotProductProject(
            (dropout): DropoutWrapper()
            (proj_1): Linear(in_features=1412, out_features=128, bias=False)
            (proj_2): Linear(in_features=1412, out_features=128, bias=False)
          )
        )
      )
      (2): AttentionWrapper(
        (score_func): SimilarityWrapper(
          (score_func): DotProductProject(
            (dropout): DropoutWrapper()
            (proj_1): Linear(in_features=1412, out_features=128, bias=False)
            (proj_2): Linear(in_features=1412, out_features=128, bias=False)
          )
        )
      )
    )
  )
  (doc_understand): OneLayerBRNN(
    (dropout): DropoutWrapper()
    (rnn): LSTM(1280, 128, bidirectional=True)
  )
  (doc_self_attn): AttentionWrapper(
    (score_func): SimilarityWrapper(
      (score_func): DotProductProject(
        (dropout): DropoutWrapper()
        (proj_1): Linear(in_features=2436, out_features=128, bias=False)
        (proj_2): Linear(in_features=2436, out_features=128, bias=False)
      )
    )
  )
  (doc_mem_gen): OneLayerBRNN(
    (dropout): DropoutWrapper()
    (rnn): LSTM(512, 128, bidirectional=True)
  )
  (query_sum_attn): SelfAttnWrapper(
    (att): LinearSelfAttn(
      (linear): Linear(in_features=256, out_features=1, bias=True)
      (dropout): DropoutWrapper()
    )
  )
  (decoder): SAN(
    (attn_b): FlatSimilarityWrapper(
      (att_dropout): DropoutWrapper()
      (score_func): BilinearFlatSim(
        (linear): Linear(in_features=256, out_features=256, bias=True)
        (dropout): DropoutWrapper()
      )
    )
    (attn_e): FlatSimilarityWrapper(
      (att_dropout): DropoutWrapper()
      (score_func): BilinearFlatSim(
        (linear): Linear(in_features=256, out_features=256, bias=True)
        (dropout): DropoutWrapper()
      )
    )
    (rnn): GRUCell(256, 256)
    (dropout): DropoutWrapper()
  )
  (doc_sum_attn): SelfAttnWrapper(
    (att): LinearSelfAttn(
      (linear): Linear(in_features=256, out_features=1, bias=True)
      (dropout): DropoutWrapper()
    )
  )
  (classifier): Classifier(
    (dropout): DropoutWrapper()
    (proj): Linear(in_features=512, out_features=1, bias=True)
  )
)
]
2018-11-21 07:40:26 INFO     [Total number of params: 9194805]
2018-11-21 07:40:44 WARNING  [At epoch 0]
2018-11-21 07:40:45 INFO     [#updates[     1] train loss[10.56736] remaining[0:18:20]]
2018-11-21 07:41:10 INFO     [#updates[   100] train loss[8.13856] remaining[0:16:53]]
2018-11-21 07:41:40 INFO     [#updates[   200] train loss[7.86389] remaining[0:17:47]]
2018-11-21 07:42:04 INFO     [#updates[   300] train loss[7.71950] remaining[0:16:40]]
2018-11-21 07:42:30 INFO     [#updates[   400] train loss[7.57996] remaining[0:16:09]]
2018-11-21 07:42:58 INFO     [#updates[   500] train loss[7.39047] remaining[0:15:51]]
2018-11-21 07:43:24 INFO     [#updates[   600] train loss[7.29136] remaining[0:15:23]]
2018-11-21 07:43:51 INFO     [#updates[   700] train loss[7.21049] remaining[0:15:00]]
2018-11-21 07:44:18 INFO     [#updates[   800] train loss[7.10791] remaining[0:14:35]]
2018-11-21 07:44:43 INFO     [#updates[   900] train loss[7.03591] remaining[0:14:00]]
2018-11-21 07:45:09 INFO     [#updates[  1000] train loss[6.96621] remaining[0:13:33]]
2018-11-21 07:45:36 INFO     [#updates[  1100] train loss[6.90393] remaining[0:13:08]]
2018-11-21 07:46:02 INFO     [#updates[  1200] train loss[6.86888] remaining[0:12:39]]
2018-11-21 07:46:26 INFO     [#updates[  1300] train loss[6.83211] remaining[0:12:08]]
2018-11-21 07:46:55 INFO     [#updates[  1400] train loss[6.76086] remaining[0:11:48]]
2018-11-21 07:47:24 INFO     [#updates[  1500] train loss[6.72364] remaining[0:11:25]]
2018-11-21 07:47:53 INFO     [#updates[  1600] train loss[6.68078] remaining[0:11:02]]
2018-11-21 07:48:19 INFO     [#updates[  1700] train loss[6.64756] remaining[0:10:35]]
2018-11-21 07:48:47 INFO     [#updates[  1800] train loss[6.62152] remaining[0:10:09]]
2018-11-21 07:49:16 INFO     [#updates[  1900] train loss[6.59858] remaining[0:09:44]]
2018-11-21 07:49:45 INFO     [#updates[  2000] train loss[6.56734] remaining[0:09:20]]
2018-11-21 07:50:11 INFO     [#updates[  2100] train loss[6.53625] remaining[0:08:52]]
2018-11-21 07:50:36 INFO     [#updates[  2200] train loss[6.51030] remaining[0:08:23]]
2018-11-21 07:51:01 INFO     [#updates[  2300] train loss[6.48831] remaining[0:07:55]]
2018-11-21 07:51:26 INFO     [#updates[  2400] train loss[6.46709] remaining[0:07:27]]
2018-11-21 07:51:51 INFO     [#updates[  2500] train loss[6.45131] remaining[0:06:59]]
2018-11-21 07:52:17 INFO     [#updates[  2600] train loss[6.42444] remaining[0:06:32]]
2018-11-21 07:52:44 INFO     [#updates[  2700] train loss[6.39191] remaining[0:06:05]]
2018-11-21 07:53:14 INFO     [#updates[  2800] train loss[6.35952] remaining[0:05:40]]
2018-11-21 07:53:40 INFO     [#updates[  2900] train loss[6.34195] remaining[0:05:13]]
2018-11-21 07:54:07 INFO     [#updates[  3000] train loss[6.31416] remaining[0:04:46]]
2018-11-21 07:54:27 INFO     [#updates[  3100] train loss[6.28242] remaining[0:04:18]]
2018-11-21 07:54:53 INFO     [#updates[  3200] train loss[6.25699] remaining[0:03:51]]
2018-11-21 07:55:19 INFO     [#updates[  3300] train loss[6.23302] remaining[0:03:24]]
2018-11-21 07:55:47 INFO     [#updates[  3400] train loss[6.20633] remaining[0:02:58]]
2018-11-21 07:56:14 INFO     [#updates[  3500] train loss[6.18180] remaining[0:02:32]]
2018-11-21 07:56:43 INFO     [#updates[  3600] train loss[6.15744] remaining[0:02:05]]
2018-11-21 07:57:08 INFO     [#updates[  3700] train loss[6.13860] remaining[0:01:39]]
2018-11-21 07:57:35 INFO     [#updates[  3800] train loss[6.11542] remaining[0:01:12]]
2018-11-21 07:58:01 INFO     [#updates[  3900] train loss[6.09288] remaining[0:00:45]]
2018-11-21 07:58:26 INFO     [#updates[  4000] train loss[6.06998] remaining[0:00:19]]
2018-11-21 07:59:32 INFO     [scheduler_type ms]
2018-11-21 07:59:34 INFO     [Saved the new best model and prediction]
2018-11-21 07:59:34 WARNING  [Epoch 0 - dev EM: 44.176 F1: 48.068 (best EM: 44.176 F1: 48.068)]
2018-11-21 07:59:34 WARNING  [Epoch 0 - ACC: 54.9987]
2018-11-21 07:59:34 WARNING  [Detailed Metric at Epoch 0: OrderedDict([('exact', 44.175861197675395), ('f1', 48.06751211805939), ('total', 11873), ('HasAns_exact', 42.577597840755736), ('HasAns_f1', 50.3720599490078), ('HasAns_total', 5928), ('NoAns_exact', 45.76955424726661), ('NoAns_f1', 45.76955424726661), ('NoAns_total', 5945)])]
2018-11-21 07:59:34 WARNING  [At epoch 1]
2018-11-21 07:59:34 INFO     [#updates[  4074] train loss[6.05309] remaining[0:11:44]]
2018-11-21 07:59:40 INFO     [#updates[  4100] train loss[6.04546] remaining[0:15:38]]
2018-11-21 08:00:03 INFO     [#updates[  4200] train loss[6.01824] remaining[0:15:07]]
2018-11-21 08:00:31 INFO     [#updates[  4300] train loss[5.99631] remaining[0:16:04]]
2018-11-21 08:00:56 INFO     [#updates[  4400] train loss[5.97202] remaining[0:15:31]]
2018-11-21 08:01:26 INFO     [#updates[  4500] train loss[5.95195] remaining[0:15:53]]
2018-11-21 08:01:49 INFO     [#updates[  4600] train loss[5.92714] remaining[0:15:05]]
2018-11-21 08:02:12 INFO     [#updates[  4700] train loss[5.90535] remaining[0:14:26]]
2018-11-21 08:02:40 INFO     [#updates[  4800] train loss[5.87857] remaining[0:14:13]]
2018-11-21 08:03:01 INFO     [#updates[  4900] train loss[5.85412] remaining[0:13:32]]
2018-11-21 08:03:29 INFO     [#updates[  5000] train loss[5.82816] remaining[0:13:17]]
2018-11-21 08:03:58 INFO     [#updates[  5100] train loss[5.81209] remaining[0:13:02]]
2018-11-21 08:04:23 INFO     [#updates[  5200] train loss[5.78987] remaining[0:12:35]]
2018-11-21 08:04:50 INFO     [#updates[  5300] train loss[5.77439] remaining[0:12:12]]
2018-11-21 08:05:17 INFO     [#updates[  5400] train loss[5.75320] remaining[0:11:49]]
2018-11-21 08:05:43 INFO     [#updates[  5500] train loss[5.73445] remaining[0:11:23]]
2018-11-21 08:06:09 INFO     [#updates[  5600] train loss[5.71558] remaining[0:10:58]]
2018-11-21 08:06:38 INFO     [#updates[  5700] train loss[5.69637] remaining[0:10:36]]
2018-11-21 08:07:04 INFO     [#updates[  5800] train loss[5.67740] remaining[0:10:10]]
2018-11-21 08:07:31 INFO     [#updates[  5900] train loss[5.65728] remaining[0:09:46]]
2018-11-21 08:08:00 INFO     [#updates[  6000] train loss[5.63898] remaining[0:09:22]]
2018-11-21 08:08:29 INFO     [#updates[  6100] train loss[5.62286] remaining[0:08:59]]
2018-11-21 08:08:58 INFO     [#updates[  6200] train loss[5.60762] remaining[0:08:35]]
2018-11-21 08:09:28 INFO     [#updates[  6300] train loss[5.59146] remaining[0:08:12]]
2018-11-21 08:09:55 INFO     [#updates[  6400] train loss[5.57544] remaining[0:07:45]]
2018-11-21 08:10:20 INFO     [#updates[  6500] train loss[5.55925] remaining[0:07:17]]
2018-11-21 08:10:49 INFO     [#updates[  6600] train loss[5.54485] remaining[0:06:52]]
2018-11-21 08:11:16 INFO     [#updates[  6700] train loss[5.52589] remaining[0:06:26]]
2018-11-21 08:11:44 INFO     [#updates[  6800] train loss[5.51022] remaining[0:06:00]]
2018-11-21 08:12:10 INFO     [#updates[  6900] train loss[5.49567] remaining[0:05:33]]
2018-11-21 08:12:40 INFO     [#updates[  7000] train loss[5.48242] remaining[0:05:07]]
2018-11-21 08:13:03 INFO     [#updates[  7100] train loss[5.46864] remaining[0:04:39]]
2018-11-21 08:13:32 INFO     [#updates[  7200] train loss[5.45372] remaining[0:04:13]]
2018-11-21 08:14:00 INFO     [#updates[  7300] train loss[5.44137] remaining[0:03:46]]
2018-11-21 08:14:25 INFO     [#updates[  7400] train loss[5.42737] remaining[0:03:19]]
2018-11-21 08:14:55 INFO     [#updates[  7500] train loss[5.41290] remaining[0:02:53]]
2018-11-21 08:15:23 INFO     [#updates[  7600] train loss[5.40057] remaining[0:02:26]]
2018-11-21 08:15:51 INFO     [#updates[  7700] train loss[5.38885] remaining[0:02:00]]
2018-11-21 08:16:20 INFO     [#updates[  7800] train loss[5.37732] remaining[0:01:33]]
2018-11-21 08:16:50 INFO     [#updates[  7900] train loss[5.36437] remaining[0:01:06]]
2018-11-21 08:17:20 INFO     [#updates[  8000] train loss[5.35233] remaining[0:00:39]]
2018-11-21 08:17:49 INFO     [#updates[  8100] train loss[5.33968] remaining[0:00:12]]
2018-11-21 08:18:45 INFO     [scheduler_type ms]
2018-11-21 08:18:48 INFO     [Saved the new best model and prediction]
2018-11-21 08:18:48 WARNING  [Epoch 1 - dev EM: 52.396 F1: 55.050 (best EM: 52.396 F1: 55.050)]
2018-11-21 08:18:48 WARNING  [Epoch 1 - ACC: 57.7697]
2018-11-21 08:18:48 WARNING  [Detailed Metric at Epoch 1: OrderedDict([('exact', 52.39619304303883), ('f1', 55.04999713367711), ('total', 11873), ('HasAns_exact', 45.58029689608637), ('HasAns_f1', 50.89551551419473), ('HasAns_total', 5928), ('NoAns_exact', 59.19259882253995), ('NoAns_f1', 59.19259882253995), ('NoAns_total', 5945)])]
2018-11-21 08:18:48 WARNING  [At epoch 2]
2018-11-21 08:18:48 INFO     [#updates[  8147] train loss[5.33402] remaining[0:10:14]]
2018-11-21 08:18:58 INFO     [#updates[  8200] train loss[5.32727] remaining[0:12:30]]
2018-11-21 08:19:18 INFO     [#updates[  8300] train loss[5.31533] remaining[0:12:42]]
2018-11-21 08:19:42 INFO     [#updates[  8400] train loss[5.30299] remaining[0:13:38]]
2018-11-21 08:20:06 INFO     [#updates[  8500] train loss[5.28941] remaining[0:13:40]]
2018-11-21 08:20:31 INFO     [#updates[  8600] train loss[5.27989] remaining[0:13:41]]
2018-11-21 08:20:56 INFO     [#updates[  8700] train loss[5.26620] remaining[0:13:33]]
2018-11-21 08:21:25 INFO     [#updates[  8800] train loss[5.25439] remaining[0:13:39]]
2018-11-21 08:21:50 INFO     [#updates[  8900] train loss[5.24372] remaining[0:13:20]]
2018-11-21 08:22:15 INFO     [#updates[  9000] train loss[5.23286] remaining[0:13:01]]
2018-11-21 08:22:45 INFO     [#updates[  9100] train loss[5.22088] remaining[0:12:53]]
2018-11-21 08:23:11 INFO     [#updates[  9200] train loss[5.20990] remaining[0:12:34]]
2018-11-21 08:23:41 INFO     [#updates[  9300] train loss[5.19886] remaining[0:12:20]]
2018-11-21 08:24:11 INFO     [#updates[  9400] train loss[5.18797] remaining[0:12:05]]
2018-11-21 08:24:38 INFO     [#updates[  9500] train loss[5.17568] remaining[0:11:43]]
2018-11-21 08:25:06 INFO     [#updates[  9600] train loss[5.16522] remaining[0:11:20]]
2018-11-21 08:25:34 INFO     [#updates[  9700] train loss[5.15610] remaining[0:10:58]]
2018-11-21 08:26:03 INFO     [#updates[  9800] train loss[5.14517] remaining[0:10:35]]
2018-11-21 08:26:31 INFO     [#updates[  9900] train loss[5.13513] remaining[0:10:11]]
2018-11-21 08:27:00 INFO     [#updates[ 10000] train loss[5.12598] remaining[0:09:48]]
2018-11-21 08:27:27 INFO     [#updates[ 10100] train loss[5.11712] remaining[0:09:22]]
2018-11-21 08:27:55 INFO     [#updates[ 10200] train loss[5.10844] remaining[0:08:57]]
2018-11-21 08:28:22 INFO     [#updates[ 10300] train loss[5.09903] remaining[0:08:31]]
2018-11-21 08:28:50 INFO     [#updates[ 10400] train loss[5.09022] remaining[0:08:05]]
2018-11-21 08:29:18 INFO     [#updates[ 10500] train loss[5.08089] remaining[0:07:39]]
2018-11-21 08:29:48 INFO     [#updates[ 10600] train loss[5.07272] remaining[0:07:15]]
2018-11-21 08:30:16 INFO     [#updates[ 10700] train loss[5.06304] remaining[0:06:49]]
2018-11-21 08:30:46 INFO     [#updates[ 10800] train loss[5.05536] remaining[0:06:23]]
2018-11-21 08:31:14 INFO     [#updates[ 10900] train loss[5.04635] remaining[0:05:57]]
2018-11-21 08:31:45 INFO     [#updates[ 11000] train loss[5.03665] remaining[0:05:31]]
2018-11-21 08:32:11 INFO     [#updates[ 11100] train loss[5.02631] remaining[0:05:04]]
2018-11-21 08:32:39 INFO     [#updates[ 11200] train loss[5.01840] remaining[0:04:37]]
2018-11-21 08:33:06 INFO     [#updates[ 11300] train loss[5.00935] remaining[0:04:10]]
2018-11-21 08:33:35 INFO     [#updates[ 11400] train loss[5.00105] remaining[0:03:43]]
2018-11-21 08:34:04 INFO     [#updates[ 11500] train loss[4.99490] remaining[0:03:16]]
2018-11-21 08:34:34 INFO     [#updates[ 11600] train loss[4.98613] remaining[0:02:49]]
2018-11-21 08:34:59 INFO     [#updates[ 11700] train loss[4.97863] remaining[0:02:21]]
2018-11-21 08:35:30 INFO     [#updates[ 11800] train loss[4.97282] remaining[0:01:54]]
2018-11-21 08:35:58 INFO     [#updates[ 11900] train loss[4.96616] remaining[0:01:27]]
2018-11-21 08:36:25 INFO     [#updates[ 12000] train loss[4.95700] remaining[0:01:00]]
2018-11-21 08:36:55 INFO     [#updates[ 12100] train loss[4.95120] remaining[0:00:32]]
2018-11-21 08:37:26 INFO     [#updates[ 12200] train loss[4.94332] remaining[0:00:05]]
2018-11-21 08:38:17 INFO     [scheduler_type ms]
2018-11-21 08:38:20 INFO     [Saved the new best model and prediction]
2018-11-21 08:38:20 WARNING  [Epoch 2 - dev EM: 54.771 F1: 57.199 (best EM: 54.771 F1: 57.199)]
2018-11-21 08:38:20 WARNING  [Epoch 2 - ACC: 57.5339]
2018-11-21 08:38:20 WARNING  [Detailed Metric at Epoch 2: OrderedDict([('exact', 54.771329908195064), ('f1', 57.19942978430953), ('total', 11873), ('HasAns_exact', 46.7442645074224), ('HasAns_f1', 51.60742743405948), ('HasAns_total', 5928), ('NoAns_exact', 62.77544154751892), ('NoAns_f1', 62.77544154751892), ('NoAns_total', 5945)])]
2018-11-21 08:38:20 WARNING  [At epoch 3]
2018-11-21 08:38:21 INFO     [#updates[ 12220] train loss[4.94257] remaining[0:10:47]]
2018-11-21 08:38:36 INFO     [#updates[ 12300] train loss[4.93620] remaining[0:12:48]]
2018-11-21 08:39:05 INFO     [#updates[ 12400] train loss[4.92936] remaining[0:16:05]]
2018-11-21 08:39:32 INFO     [#updates[ 12500] train loss[4.92060] remaining[0:16:07]]
2018-11-21 08:40:00 INFO     [#updates[ 12600] train loss[4.91292] remaining[0:16:02]]
2018-11-21 08:40:25 INFO     [#updates[ 12700] train loss[4.90559] remaining[0:15:32]]
2018-11-21 08:40:53 INFO     [#updates[ 12800] train loss[4.89808] remaining[0:15:17]]
2018-11-21 08:41:21 INFO     [#updates[ 12900] train loss[4.88973] remaining[0:14:57]]
2018-11-21 08:41:50 INFO     [#updates[ 13000] train loss[4.88247] remaining[0:14:44]]
2018-11-21 08:42:18 INFO     [#updates[ 13100] train loss[4.87528] remaining[0:14:21]]
2018-11-21 08:42:46 INFO     [#updates[ 13200] train loss[4.86908] remaining[0:13:56]]
2018-11-21 08:43:15 INFO     [#updates[ 13300] train loss[4.86224] remaining[0:13:35]]
2018-11-21 08:43:43 INFO     [#updates[ 13400] train loss[4.85521] remaining[0:13:09]]
2018-11-21 08:44:10 INFO     [#updates[ 13500] train loss[4.84868] remaining[0:12:41]]
2018-11-21 08:44:39 INFO     [#updates[ 13600] train loss[4.84127] remaining[0:12:18]]
2018-11-21 08:45:09 INFO     [#updates[ 13700] train loss[4.83439] remaining[0:11:55]]
2018-11-21 08:45:40 INFO     [#updates[ 13800] train loss[4.82829] remaining[0:11:32]]
2018-11-21 08:46:08 INFO     [#updates[ 13900] train loss[4.82206] remaining[0:11:05]]
2018-11-21 08:46:40 INFO     [#updates[ 14000] train loss[4.81598] remaining[0:10:42]]
2018-11-21 08:47:07 INFO     [#updates[ 14100] train loss[4.80966] remaining[0:10:13]]
2018-11-21 08:47:34 INFO     [#updates[ 14200] train loss[4.80388] remaining[0:09:44]]
2018-11-21 08:48:00 INFO     [#updates[ 14300] train loss[4.79679] remaining[0:09:14]]
2018-11-21 08:48:25 INFO     [#updates[ 14400] train loss[4.79023] remaining[0:08:43]]
2018-11-21 08:48:55 INFO     [#updates[ 14500] train loss[4.78270] remaining[0:08:18]]
2018-11-21 08:49:24 INFO     [#updates[ 14600] train loss[4.77792] remaining[0:07:51]]
2018-11-21 08:49:56 INFO     [#updates[ 14700] train loss[4.77157] remaining[0:07:26]]
2018-11-21 08:50:22 INFO     [#updates[ 14800] train loss[4.76536] remaining[0:06:57]]
2018-11-21 08:50:53 INFO     [#updates[ 14900] train loss[4.75977] remaining[0:06:30]]
2018-11-21 08:51:22 INFO     [#updates[ 15000] train loss[4.75375] remaining[0:06:02]]
2018-11-21 08:51:51 INFO     [#updates[ 15100] train loss[4.74829] remaining[0:05:35]]
2018-11-21 08:52:20 INFO     [#updates[ 15200] train loss[4.74352] remaining[0:05:07]]
2018-11-21 08:52:47 INFO     [#updates[ 15300] train loss[4.73642] remaining[0:04:39]]
2018-11-21 08:53:18 INFO     [#updates[ 15400] train loss[4.73197] remaining[0:04:11]]
2018-11-21 08:53:48 INFO     [#updates[ 15500] train loss[4.72668] remaining[0:03:43]]
2018-11-21 08:54:18 INFO     [#updates[ 15600] train loss[4.72119] remaining[0:03:15]]
2018-11-21 08:54:48 INFO     [#updates[ 15700] train loss[4.71601] remaining[0:02:47]]
2018-11-21 08:55:17 INFO     [#updates[ 15800] train loss[4.71058] remaining[0:02:19]]
2018-11-21 08:55:44 INFO     [#updates[ 15900] train loss[4.70593] remaining[0:01:51]]
2018-11-21 08:56:10 INFO     [#updates[ 16000] train loss[4.69936] remaining[0:01:22]]
2018-11-21 08:56:41 INFO     [#updates[ 16100] train loss[4.69439] remaining[0:00:54]]
2018-11-21 08:57:08 INFO     [#updates[ 16200] train loss[4.68982] remaining[0:00:26]]
2018-11-21 08:58:16 INFO     [scheduler_type ms]
2018-11-21 08:58:18 INFO     [Saved the new best model and prediction]
2018-11-21 08:58:18 WARNING  [Epoch 3 - dev EM: 58.806 F1: 60.494 (best EM: 58.806 F1: 60.494)]
2018-11-21 08:58:18 WARNING  [Epoch 3 - ACC: 61.8631]
2018-11-21 08:58:18 WARNING  [Detailed Metric at Epoch 3: OrderedDict([('exact', 58.80569359049945), ('f1', 60.49428353913337), ('total', 11873), ('HasAns_exact', 42.51012145748988), ('HasAns_f1', 45.89214380231607), ('HasAns_total', 5928), ('NoAns_exact', 75.05466778805719), ('NoAns_f1', 75.05466778805719), ('NoAns_total', 5945)])]
2018-11-21 08:58:18 WARNING  [At epoch 4]
2018-11-21 08:58:19 INFO     [#updates[ 16293] train loss[4.68550] remaining[0:15:00]]
2018-11-21 08:58:20 INFO     [#updates[ 16300] train loss[4.68509] remaining[0:13:03]]
2018-11-21 08:58:40 INFO     [#updates[ 16400] train loss[4.67971] remaining[0:13:02]]
2018-11-21 08:59:07 INFO     [#updates[ 16500] train loss[4.67559] remaining[0:15:10]]
2018-11-21 08:59:34 INFO     [#updates[ 16600] train loss[4.67053] remaining[0:15:23]]
2018-11-21 09:00:01 INFO     [#updates[ 16700] train loss[4.66465] remaining[0:15:20]]
2018-11-21 09:00:30 INFO     [#updates[ 16800] train loss[4.65979] remaining[0:15:23]]
2018-11-21 09:00:57 INFO     [#updates[ 16900] train loss[4.65333] remaining[0:15:02]]
2018-11-21 09:01:27 INFO     [#updates[ 17000] train loss[4.64862] remaining[0:14:57]]
2018-11-21 09:02:00 INFO     [#updates[ 17100] train loss[4.64466] remaining[0:14:54]]
2018-11-21 09:02:29 INFO     [#updates[ 17200] train loss[4.63948] remaining[0:14:33]]
2018-11-21 09:03:00 INFO     [#updates[ 17300] train loss[4.63376] remaining[0:14:14]]
2018-11-21 09:03:29 INFO     [#updates[ 17400] train loss[4.62842] remaining[0:13:50]]
2018-11-21 09:03:57 INFO     [#updates[ 17500] train loss[4.62274] remaining[0:13:22]]
2018-11-21 09:04:26 INFO     [#updates[ 17600] train loss[4.61737] remaining[0:12:57]]
2018-11-21 09:04:52 INFO     [#updates[ 17700] train loss[4.61328] remaining[0:12:25]]
2018-11-21 09:05:22 INFO     [#updates[ 17800] train loss[4.60817] remaining[0:12:01]]
2018-11-21 09:05:53 INFO     [#updates[ 17900] train loss[4.60313] remaining[0:11:37]]
2018-11-21 09:06:23 INFO     [#updates[ 18000] train loss[4.59798] remaining[0:11:10]]
2018-11-21 09:06:52 INFO     [#updates[ 18100] train loss[4.59287] remaining[0:10:43]]
2018-11-21 09:07:20 INFO     [#updates[ 18200] train loss[4.58758] remaining[0:10:14]]
2018-11-21 09:07:51 INFO     [#updates[ 18300] train loss[4.58389] remaining[0:09:48]]
2018-11-21 09:08:17 INFO     [#updates[ 18400] train loss[4.57897] remaining[0:09:18]]
2018-11-21 09:08:47 INFO     [#updates[ 18500] train loss[4.57441] remaining[0:08:50]]
2018-11-21 09:09:11 INFO     [#updates[ 18600] train loss[4.56964] remaining[0:08:18]]
2018-11-21 09:09:39 INFO     [#updates[ 18700] train loss[4.56567] remaining[0:07:50]]
2018-11-21 09:10:06 INFO     [#updates[ 18800] train loss[4.56112] remaining[0:07:21]]
2018-11-21 09:10:32 INFO     [#updates[ 18900] train loss[4.55666] remaining[0:06:51]]
2018-11-21 09:10:59 INFO     [#updates[ 19000] train loss[4.55280] remaining[0:06:23]]
2018-11-21 09:11:26 INFO     [#updates[ 19100] train loss[4.54875] remaining[0:05:54]]
2018-11-21 09:11:58 INFO     [#updates[ 19200] train loss[4.54356] remaining[0:05:28]]
2018-11-21 09:12:26 INFO     [#updates[ 19300] train loss[4.53971] remaining[0:05:00]]
2018-11-21 09:12:56 INFO     [#updates[ 19400] train loss[4.53640] remaining[0:04:32]]
2018-11-21 09:13:25 INFO     [#updates[ 19500] train loss[4.53276] remaining[0:04:04]]
2018-11-21 09:13:55 INFO     [#updates[ 19600] train loss[4.52861] remaining[0:03:36]]
2018-11-21 09:14:24 INFO     [#updates[ 19700] train loss[4.52477] remaining[0:03:08]]
2018-11-21 09:14:54 INFO     [#updates[ 19800] train loss[4.52064] remaining[0:02:40]]
2018-11-21 09:15:24 INFO     [#updates[ 19900] train loss[4.51736] remaining[0:02:12]]
2018-11-21 09:15:54 INFO     [#updates[ 20000] train loss[4.51389] remaining[0:01:43]]
2018-11-21 09:16:23 INFO     [#updates[ 20100] train loss[4.50911] remaining[0:01:15]]
2018-11-21 09:16:53 INFO     [#updates[ 20200] train loss[4.50471] remaining[0:00:47]]
2018-11-21 09:17:24 INFO     [#updates[ 20300] train loss[4.50049] remaining[0:00:18]]
2018-11-21 09:18:31 INFO     [scheduler_type ms]
2018-11-21 09:18:33 INFO     [Saved the new best model and prediction]
2018-11-21 09:18:33 WARNING  [Epoch 4 - dev EM: 58.755 F1: 60.938 (best EM: 58.755 F1: 60.938)]
2018-11-21 09:18:33 WARNING  [Epoch 4 - ACC: 61.8294]
2018-11-21 09:18:33 WARNING  [Detailed Metric at Epoch 4: OrderedDict([('exact', 58.755158763581235), ('f1', 60.938401844372535), ('total', 11873), ('HasAns_exact', 51.72064777327935), ('HasAns_f1', 56.093394922104125), ('HasAns_total', 5928), ('NoAns_exact', 65.76955424726661), ('NoAns_f1', 65.76955424726661), ('NoAns_total', 5945)])]

You may want to consider changing your batch submission script as follows to speed up your job run next time:

#PBS -l select=1:ncpus=28:mem=6gb
#PBS -l place=free:shared
#PBS -l walltime=01:44:00

Your group nirav has been charged 01:39:04 for 28 cpus.
You previously had 4853:53:34.  You now have 4807:39:42 remaining for the queue oc_standard
