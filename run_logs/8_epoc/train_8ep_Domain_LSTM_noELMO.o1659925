2018-11-25 11:40:35 INFO     [Launching the SAN]
2018-11-25 11:40:35 INFO     [Loading data]
Loaded 130319 samples out of 130319
Loaded 11873 samples out of 11873
2018-11-25 11:41:08 INFO     [
############# Model Arch of SAN #############
DNetwork(
  (dropout): DropoutWrapper()
  (lexicon_encoder): LexiconEncoder(
    (dropout): DropoutWrapper()
    (dropout_emb): DropoutWrapper()
    (dropout_cove): DropoutWrapper()
    (embedding): Embedding(90953, 300, padding_idx=0)
    (ContextualEmbed): ContextualEmbed(
      (embedding): Embedding(90953, 300, padding_idx=0)
      (rnn1): LSTM(300, 300, bidirectional=True)
      (rnn2): LSTM(600, 300, bidirectional=True)
    )
    (prealign): AttentionWrapper(
      (score_func): SimilarityWrapper(
        (score_func): DotProductProject(
          (dropout): DropoutWrapper()
          (proj_1): Linear(in_features=300, out_features=128, bias=False)
          (proj_2): Linear(in_features=300, out_features=128, bias=False)
        )
      )
    )
    (pos_embedding): Embedding(54, 12, padding_idx=0)
    (ner_embedding): Embedding(41, 8, padding_idx=0)
    (doc_pwnn): PositionwiseNN(
      (w_0): Conv1d(1224, 256, kernel_size=(1,), stride=(1,))
      (w_1): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
      (dropout): DropoutWrapper()
    )
    (que_pwnn): PositionwiseNN(
      (w_0): Conv1d(900, 256, kernel_size=(1,), stride=(1,))
      (w_1): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
      (dropout): DropoutWrapper()
    )
  )
  (doc_encoder_low): OneLayerBRNN(
    (dropout): DropoutWrapper()
    (rnn): LSTM(856, 128, bidirectional=True)
  )
  (doc_encoder_high): OneLayerBRNN(
    (dropout): DropoutWrapper()
    (rnn): LSTM(856, 128, bidirectional=True)
  )
  (query_encoder_low): OneLayerBRNN(
    (dropout): DropoutWrapper()
    (rnn): LSTM(856, 128, bidirectional=True)
  )
  (query_encoder_high): OneLayerBRNN(
    (dropout): DropoutWrapper()
    (rnn): LSTM(856, 128, bidirectional=True)
  )
  (query_understand): OneLayerBRNN(
    (dropout): DropoutWrapper()
    (rnn): LSTM(512, 128, bidirectional=True)
  )
  (deep_attn): DeepAttentionWrapper(
    (dropout): DropoutWrapper()
    (attn_list): ModuleList(
      (0): AttentionWrapper(
        (score_func): SimilarityWrapper(
          (score_func): DotProductProject(
            (dropout): DropoutWrapper()
            (proj_1): Linear(in_features=1412, out_features=128, bias=False)
            (proj_2): Linear(in_features=1412, out_features=128, bias=False)
          )
        )
      )
      (1): AttentionWrapper(
        (score_func): SimilarityWrapper(
          (score_func): DotProductProject(
            (dropout): DropoutWrapper()
            (proj_1): Linear(in_features=1412, out_features=128, bias=False)
            (proj_2): Linear(in_features=1412, out_features=128, bias=False)
          )
        )
      )
      (2): AttentionWrapper(
        (score_func): SimilarityWrapper(
          (score_func): DotProductProject(
            (dropout): DropoutWrapper()
            (proj_1): Linear(in_features=1412, out_features=128, bias=False)
            (proj_2): Linear(in_features=1412, out_features=128, bias=False)
          )
        )
      )
    )
  )
  (doc_understand): OneLayerBRNN(
    (dropout): DropoutWrapper()
    (rnn): LSTM(1280, 128, bidirectional=True)
  )
  (doc_self_attn): AttentionWrapper(
    (score_func): SimilarityWrapper(
      (score_func): DotProductProject(
        (dropout): DropoutWrapper()
        (proj_1): Linear(in_features=2436, out_features=128, bias=False)
        (proj_2): Linear(in_features=2436, out_features=128, bias=False)
      )
    )
  )
  (doc_mem_gen): OneLayerBRNN(
    (dropout): DropoutWrapper()
    (rnn): LSTM(512, 128, bidirectional=True)
  )
  (query_domain_mem): OneLayerBRNN(
    (dropout): DropoutWrapper()
    (rnn): LSTM(768, 128, bidirectional=True)
  )
  (query_sum_attn): SelfAttnWrapper(
    (att): LinearSelfAttn(
      (linear): Linear(in_features=256, out_features=1, bias=True)
      (dropout): DropoutWrapper()
    )
  )
  (decoder): SAN(
    (attn_b): FlatSimilarityWrapper(
      (att_dropout): DropoutWrapper()
      (score_func): BilinearFlatSim(
        (linear): Linear(in_features=256, out_features=256, bias=True)
        (dropout): DropoutWrapper()
      )
    )
    (attn_e): FlatSimilarityWrapper(
      (att_dropout): DropoutWrapper()
      (score_func): BilinearFlatSim(
        (linear): Linear(in_features=256, out_features=256, bias=True)
        (dropout): DropoutWrapper()
      )
    )
    (rnn): GRUCell(256, 256)
    (dropout): DropoutWrapper()
  )
  (doc_sum_attn): SelfAttnWrapper(
    (att): LinearSelfAttn(
      (linear): Linear(in_features=256, out_features=1, bias=True)
      (dropout): DropoutWrapper()
    )
  )
  (classifier): Classifier(
    (dropout): DropoutWrapper()
    (proj): Linear(in_features=512, out_features=1, bias=True)
  )
)
]
2018-11-25 11:41:09 INFO     [Total number of params: 10114357]
2018-11-25 11:41:19 WARNING  [At epoch 0]
2018-11-25 11:41:19 INFO     [#updates[     1] train loss[10.84544] remaining[0:17:55]]
2018-11-25 11:41:47 INFO     [#updates[   100] train loss[6.77676] remaining[0:18:37]]
2018-11-25 11:42:13 INFO     [#updates[   200] train loss[6.41168] remaining[0:17:25]]
2018-11-25 11:42:41 INFO     [#updates[   300] train loss[6.23422] remaining[0:17:10]]
2018-11-25 11:43:07 INFO     [#updates[   400] train loss[6.07870] remaining[0:16:30]]
2018-11-25 11:43:35 INFO     [#updates[   500] train loss[5.88602] remaining[0:16:15]]
2018-11-25 11:44:05 INFO     [#updates[   600] train loss[5.81398] remaining[0:15:59]]
2018-11-25 11:44:34 INFO     [#updates[   700] train loss[5.73849] remaining[0:15:40]]
2018-11-25 11:45:03 INFO     [#updates[   800] train loss[5.64159] remaining[0:15:18]]
2018-11-25 11:45:32 INFO     [#updates[   900] train loss[5.58124] remaining[0:14:53]]
2018-11-25 11:46:00 INFO     [#updates[  1000] train loss[5.52605] remaining[0:14:22]]
2018-11-25 11:46:29 INFO     [#updates[  1100] train loss[5.46559] remaining[0:13:57]]
2018-11-25 11:46:58 INFO     [#updates[  1200] train loss[5.45278] remaining[0:13:31]]
2018-11-25 11:47:26 INFO     [#updates[  1300] train loss[5.43245] remaining[0:13:02]]
2018-11-25 11:47:54 INFO     [#updates[  1400] train loss[5.36668] remaining[0:12:34]]
2018-11-25 11:48:20 INFO     [#updates[  1500] train loss[5.33710] remaining[0:12:03]]
2018-11-25 11:48:54 INFO     [#updates[  1600] train loss[5.29764] remaining[0:11:43]]
2018-11-25 11:49:23 INFO     [#updates[  1700] train loss[5.27008] remaining[0:11:16]]
2018-11-25 11:49:53 INFO     [#updates[  1800] train loss[5.24803] remaining[0:10:49]]
2018-11-25 11:50:23 INFO     [#updates[  1900] train loss[5.23670] remaining[0:10:21]]
2018-11-25 11:50:52 INFO     [#updates[  2000] train loss[5.21495] remaining[0:09:53]]
2018-11-25 11:51:20 INFO     [#updates[  2100] train loss[5.18989] remaining[0:09:25]]
2018-11-25 11:51:53 INFO     [#updates[  2200] train loss[5.17558] remaining[0:08:59]]
2018-11-25 11:52:21 INFO     [#updates[  2300] train loss[5.16195] remaining[0:08:30]]
2018-11-25 11:52:50 INFO     [#updates[  2400] train loss[5.15193] remaining[0:08:01]]
2018-11-25 11:53:19 INFO     [#updates[  2500] train loss[5.15079] remaining[0:07:33]]
2018-11-25 11:53:47 INFO     [#updates[  2600] train loss[5.13423] remaining[0:07:04]]
2018-11-25 11:54:15 INFO     [#updates[  2700] train loss[5.10986] remaining[0:06:34]]
2018-11-25 11:54:51 INFO     [#updates[  2800] train loss[5.08454] remaining[0:06:09]]
2018-11-25 11:55:15 INFO     [#updates[  2900] train loss[5.07917] remaining[0:05:38]]
2018-11-25 11:55:46 INFO     [#updates[  3000] train loss[5.06427] remaining[0:05:10]]
2018-11-25 11:56:15 INFO     [#updates[  3100] train loss[5.04048] remaining[0:04:41]]
2018-11-25 11:56:42 INFO     [#updates[  3200] train loss[5.02365] remaining[0:04:11]]
2018-11-25 11:57:12 INFO     [#updates[  3300] train loss[5.01132] remaining[0:03:43]]
2018-11-25 11:57:43 INFO     [#updates[  3400] train loss[5.00065] remaining[0:03:14]]
2018-11-25 11:58:12 INFO     [#updates[  3500] train loss[4.98947] remaining[0:02:45]]
2018-11-25 11:58:43 INFO     [#updates[  3600] train loss[4.97993] remaining[0:02:17]]
2018-11-25 11:59:15 INFO     [#updates[  3700] train loss[4.97039] remaining[0:01:48]]
2018-11-25 11:59:45 INFO     [#updates[  3800] train loss[4.95684] remaining[0:01:19]]
2018-11-26 12:00:12 INFO     [#updates[  3900] train loss[4.94686] remaining[0:00:50]]
2018-11-26 12:00:40 INFO     [#updates[  4000] train loss[4.93403] remaining[0:00:21]]
2018-11-26 12:01:44 INFO     [scheduler_type ms]
2018-11-26 12:01:48 INFO     [Saved the new best model and prediction]
2018-11-26 12:01:48 WARNING  [Epoch 0 - dev EM: 18.849 F1: 24.042 (best EM: 18.849 F1: 24.042)]
2018-11-26 12:01:48 WARNING  [Epoch 0 - ACC: 49.9200]
2018-11-26 12:01:48 WARNING  [Detailed Metric at Epoch 0: OrderedDict([('exact', 18.84949044049524), ('f1', 24.04231465528224), ('total', 11873), ('HasAns_exact', 37.41565452091768), ('HasAns_f1', 47.816194652862045), ('HasAns_total', 5928), ('NoAns_exact', 0.33641715727502103), ('NoAns_f1', 0.33641715727502103), ('NoAns_total', 5945)])]
2018-11-26 12:01:48 WARNING  [At epoch 1]
2018-11-26 12:01:48 INFO     [#updates[  4074] train loss[4.92720] remaining[0:12:42]]
2018-11-26 12:01:54 INFO     [#updates[  4100] train loss[4.91952] remaining[0:15:24]]
2018-11-26 12:02:23 INFO     [#updates[  4200] train loss[4.90509] remaining[0:18:03]]
2018-11-26 12:02:53 INFO     [#updates[  4300] train loss[4.89214] remaining[0:18:26]]
2018-11-26 12:03:19 INFO     [#updates[  4400] train loss[4.87708] remaining[0:17:31]]
2018-11-26 12:03:51 INFO     [#updates[  4500] train loss[4.86281] remaining[0:17:35]]
2018-11-26 12:04:19 INFO     [#updates[  4600] train loss[4.84581] remaining[0:16:55]]
2018-11-26 12:04:47 INFO     [#updates[  4700] train loss[4.83049] remaining[0:16:22]]
2018-11-26 12:05:16 INFO     [#updates[  4800] train loss[4.80734] remaining[0:15:59]]
2018-11-26 12:05:46 INFO     [#updates[  4900] train loss[4.78920] remaining[0:15:34]]
2018-11-26 12:06:13 INFO     [#updates[  5000] train loss[4.76530] remaining[0:15:01]]
2018-11-26 12:06:48 INFO     [#updates[  5100] train loss[4.75185] remaining[0:14:50]]
2018-11-26 12:07:16 INFO     [#updates[  5200] train loss[4.73231] remaining[0:14:17]]
2018-11-26 12:07:48 INFO     [#updates[  5300] train loss[4.72274] remaining[0:13:55]]
2018-11-26 12:08:16 INFO     [#updates[  5400] train loss[4.70506] remaining[0:13:23]]
2018-11-26 12:08:45 INFO     [#updates[  5500] train loss[4.68697] remaining[0:12:53]]
2018-11-26 12:09:17 INFO     [#updates[  5600] train loss[4.66659] remaining[0:12:29]]
2018-11-26 12:09:46 INFO     [#updates[  5700] train loss[4.64743] remaining[0:11:59]]
2018-11-26 12:10:17 INFO     [#updates[  5800] train loss[4.62988] remaining[0:11:32]]
2018-11-26 12:10:46 INFO     [#updates[  5900] train loss[4.61083] remaining[0:11:01]]
2018-11-26 12:11:13 INFO     [#updates[  6000] train loss[4.59100] remaining[0:10:29]]
2018-11-26 12:11:40 INFO     [#updates[  6100] train loss[4.57747] remaining[0:09:58]]
2018-11-26 12:12:09 INFO     [#updates[  6200] train loss[4.56472] remaining[0:09:28]]
2018-11-26 12:12:38 INFO     [#updates[  6300] train loss[4.54799] remaining[0:08:59]]
2018-11-26 12:13:09 INFO     [#updates[  6400] train loss[4.53090] remaining[0:08:31]]
2018-11-26 12:13:39 INFO     [#updates[  6500] train loss[4.51374] remaining[0:08:02]]
2018-11-26 12:14:06 INFO     [#updates[  6600] train loss[4.49977] remaining[0:07:31]]
2018-11-26 12:14:40 INFO     [#updates[  6700] train loss[4.47982] remaining[0:07:05]]
2018-11-26 12:15:08 INFO     [#updates[  6800] train loss[4.46206] remaining[0:06:35]]
2018-11-26 12:15:30 INFO     [#updates[  6900] train loss[4.44684] remaining[0:06:02]]
2018-11-26 12:16:02 INFO     [#updates[  7000] train loss[4.43427] remaining[0:05:34]]
2018-11-26 12:16:32 INFO     [#updates[  7100] train loss[4.42146] remaining[0:05:05]]
2018-11-26 12:17:05 INFO     [#updates[  7200] train loss[4.40674] remaining[0:04:37]]
2018-11-26 12:17:36 INFO     [#updates[  7300] train loss[4.39460] remaining[0:04:08]]
2018-11-26 12:18:07 INFO     [#updates[  7400] train loss[4.38115] remaining[0:03:39]]
2018-11-26 12:18:35 INFO     [#updates[  7500] train loss[4.36566] remaining[0:03:09]]
2018-11-26 12:19:06 INFO     [#updates[  7600] train loss[4.35236] remaining[0:02:40]]
2018-11-26 12:19:36 INFO     [#updates[  7700] train loss[4.33965] remaining[0:02:11]]
2018-11-26 12:20:03 INFO     [#updates[  7800] train loss[4.32841] remaining[0:01:41]]
2018-11-26 12:20:32 INFO     [#updates[  7900] train loss[4.31402] remaining[0:01:12]]
2018-11-26 12:21:00 INFO     [#updates[  8000] train loss[4.30043] remaining[0:00:42]]
2018-11-26 12:21:33 INFO     [#updates[  8100] train loss[4.28630] remaining[0:00:13]]
2018-11-26 12:22:32 INFO     [scheduler_type ms]
2018-11-26 12:22:37 INFO     [Saved the new best model and prediction]
2018-11-26 12:22:37 WARNING  [Epoch 1 - dev EM: 30.355 F1: 35.564 (best EM: 30.355 F1: 35.564)]
2018-11-26 12:22:37 WARNING  [Epoch 1 - ACC: 49.8695]
2018-11-26 12:22:37 WARNING  [Detailed Metric at Epoch 1: OrderedDict([('exact', 30.354586035542827), ('f1', 35.563658224572514), ('total', 11873), ('HasAns_exact', 58.24898785425101), ('HasAns_f1', 68.68207052974856), ('HasAns_total', 5928), ('NoAns_exact', 2.5399495374264087), ('NoAns_f1', 2.5399495374264087), ('NoAns_total', 5945)])]
2018-11-26 12:22:37 WARNING  [At epoch 2]
2018-11-26 12:22:37 INFO     [#updates[  8147] train loss[4.27997] remaining[0:10:59]]
2018-11-26 12:22:50 INFO     [#updates[  8200] train loss[4.27229] remaining[0:16:11]]
2018-11-26 12:23:17 INFO     [#updates[  8300] train loss[4.25905] remaining[0:17:14]]
2018-11-26 12:23:49 INFO     [#updates[  8400] train loss[4.24693] remaining[0:18:08]]
2018-11-26 12:24:20 INFO     [#updates[  8500] train loss[4.23323] remaining[0:18:02]]
2018-11-26 12:24:50 INFO     [#updates[  8600] train loss[4.22352] remaining[0:17:47]]
2018-11-26 12:25:13 INFO     [#updates[  8700] train loss[4.20894] remaining[0:16:30]]
2018-11-26 12:25:43 INFO     [#updates[  8800] train loss[4.19576] remaining[0:16:16]]
2018-11-26 12:26:11 INFO     [#updates[  8900] train loss[4.18490] remaining[0:15:43]]
2018-11-26 12:26:42 INFO     [#updates[  9000] train loss[4.17343] remaining[0:15:26]]
2018-11-26 12:27:16 INFO     [#updates[  9100] train loss[4.16227] remaining[0:15:12]]
2018-11-26 12:27:45 INFO     [#updates[  9200] train loss[4.15042] remaining[0:14:42]]
2018-11-26 12:28:14 INFO     [#updates[  9300] train loss[4.13853] remaining[0:14:13]]
2018-11-26 12:28:46 INFO     [#updates[  9400] train loss[4.12734] remaining[0:13:50]]
2018-11-26 12:29:17 INFO     [#updates[  9500] train loss[4.11462] remaining[0:13:23]]
2018-11-26 12:29:48 INFO     [#updates[  9600] train loss[4.10324] remaining[0:12:56]]
2018-11-26 12:30:19 INFO     [#updates[  9700] train loss[4.09469] remaining[0:12:29]]
2018-11-26 12:30:51 INFO     [#updates[  9800] train loss[4.08376] remaining[0:12:02]]
2018-11-26 12:31:20 INFO     [#updates[  9900] train loss[nan] remaining[0:11:31]]
2018-11-26 12:31:47 INFO     [#updates[ 10000] train loss[nan] remaining[0:10:58]]
2018-11-26 12:32:13 INFO     [#updates[ 10100] train loss[nan] remaining[0:10:24]]
2018-11-26 12:32:41 INFO     [#updates[ 10200] train loss[nan] remaining[0:09:53]]
2018-11-26 12:33:11 INFO     [#updates[ 10300] train loss[nan] remaining[0:09:25]]
2018-11-26 12:33:32 INFO     [#updates[ 10400] train loss[nan] remaining[0:08:48]]
2018-11-26 12:34:02 INFO     [#updates[ 10500] train loss[nan] remaining[0:08:20]]
2018-11-26 12:34:34 INFO     [#updates[ 10600] train loss[nan] remaining[0:07:53]]
2018-11-26 12:35:06 INFO     [#updates[ 10700] train loss[nan] remaining[0:07:25]]
2018-11-26 12:35:37 INFO     [#updates[ 10800] train loss[nan] remaining[0:06:57]]
2018-11-26 12:36:10 INFO     [#updates[ 10900] train loss[nan] remaining[0:06:29]]
2018-11-26 12:36:40 INFO     [#updates[ 11000] train loss[nan] remaining[0:06:00]]
2018-11-26 12:37:07 INFO     [#updates[ 11100] train loss[nan] remaining[0:05:29]]
2018-11-26 12:37:37 INFO     [#updates[ 11200] train loss[nan] remaining[0:05:00]]
2018-11-26 12:38:08 INFO     [#updates[ 11300] train loss[nan] remaining[0:04:31]]
2018-11-26 12:38:39 INFO     [#updates[ 11400] train loss[nan] remaining[0:04:02]]
2018-11-26 12:39:08 INFO     [#updates[ 11500] train loss[nan] remaining[0:03:32]]
2018-11-26 12:39:38 INFO     [#updates[ 11600] train loss[nan] remaining[0:03:03]]
2018-11-26 12:40:05 INFO     [#updates[ 11700] train loss[nan] remaining[0:02:33]]
2018-11-26 12:40:31 INFO     [#updates[ 11800] train loss[nan] remaining[0:02:03]]
2018-11-26 12:41:00 INFO     [#updates[ 11900] train loss[nan] remaining[0:01:33]]
2018-11-26 12:41:30 INFO     [#updates[ 12000] train loss[nan] remaining[0:01:04]]
2018-11-26 12:41:59 INFO     [#updates[ 12100] train loss[nan] remaining[0:00:34]]
2018-11-26 12:42:27 INFO     [#updates[ 12200] train loss[nan] remaining[0:00:05]]

You may want to consider changing your batch submission script as follows to speed up your job run next time:

#PBS -l select=1:ncpus=28:mem=6gb
#PBS -l place=free:shared
#PBS -l walltime=01:07:00

Your group changxuwu has been charged 01:02:50 for 28 cpus.
You previously had 22557:56:44.  You now have 22528:37:24 remaining for the queue oc_standard
