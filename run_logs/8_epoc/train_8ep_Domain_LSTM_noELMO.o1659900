2018-11-25 10:05:42 INFO     [Launching the SAN]
2018-11-25 10:05:42 INFO     [Loading data]
Loaded 130319 samples out of 130319
Loaded 11873 samples out of 11873
2018-11-25 10:06:07 INFO     [
############# Model Arch of SAN #############
DNetwork(
  (dropout): DropoutWrapper()
  (lexicon_encoder): LexiconEncoder(
    (dropout): DropoutWrapper()
    (dropout_emb): DropoutWrapper()
    (dropout_cove): DropoutWrapper()
    (embedding): Embedding(90953, 300, padding_idx=0)
    (ContextualEmbed): ContextualEmbed(
      (embedding): Embedding(90953, 300, padding_idx=0)
      (rnn1): LSTM(300, 300, bidirectional=True)
      (rnn2): LSTM(600, 300, bidirectional=True)
    )
    (prealign): AttentionWrapper(
      (score_func): SimilarityWrapper(
        (score_func): DotProductProject(
          (dropout): DropoutWrapper()
          (proj_1): Linear(in_features=300, out_features=128, bias=False)
          (proj_2): Linear(in_features=300, out_features=128, bias=False)
        )
      )
    )
    (pos_embedding): Embedding(54, 12, padding_idx=0)
    (ner_embedding): Embedding(41, 8, padding_idx=0)
    (doc_pwnn): PositionwiseNN(
      (w_0): Conv1d(1224, 256, kernel_size=(1,), stride=(1,))
      (w_1): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
      (dropout): DropoutWrapper()
    )
    (que_pwnn): PositionwiseNN(
      (w_0): Conv1d(900, 256, kernel_size=(1,), stride=(1,))
      (w_1): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
      (dropout): DropoutWrapper()
    )
  )
  (doc_encoder_low): OneLayerBRNN(
    (dropout): DropoutWrapper()
    (rnn): LSTM(856, 128, bidirectional=True)
  )
  (doc_encoder_high): OneLayerBRNN(
    (dropout): DropoutWrapper()
    (rnn): LSTM(856, 128, bidirectional=True)
  )
  (query_encoder_low): OneLayerBRNN(
    (dropout): DropoutWrapper()
    (rnn): LSTM(856, 128, bidirectional=True)
  )
  (query_encoder_high): OneLayerBRNN(
    (dropout): DropoutWrapper()
    (rnn): LSTM(856, 128, bidirectional=True)
  )
  (query_understand): OneLayerBRNN(
    (dropout): DropoutWrapper()
    (rnn): LSTM(512, 128, bidirectional=True)
  )
  (deep_attn): DeepAttentionWrapper(
    (dropout): DropoutWrapper()
    (attn_list): ModuleList(
      (0): AttentionWrapper(
        (score_func): SimilarityWrapper(
          (score_func): DotProductProject(
            (dropout): DropoutWrapper()
            (proj_1): Linear(in_features=1412, out_features=128, bias=False)
            (proj_2): Linear(in_features=1412, out_features=128, bias=False)
          )
        )
      )
      (1): AttentionWrapper(
        (score_func): SimilarityWrapper(
          (score_func): DotProductProject(
            (dropout): DropoutWrapper()
            (proj_1): Linear(in_features=1412, out_features=128, bias=False)
            (proj_2): Linear(in_features=1412, out_features=128, bias=False)
          )
        )
      )
      (2): AttentionWrapper(
        (score_func): SimilarityWrapper(
          (score_func): DotProductProject(
            (dropout): DropoutWrapper()
            (proj_1): Linear(in_features=1412, out_features=128, bias=False)
            (proj_2): Linear(in_features=1412, out_features=128, bias=False)
          )
        )
      )
    )
  )
  (doc_understand): OneLayerBRNN(
    (dropout): DropoutWrapper()
    (rnn): LSTM(1280, 128, bidirectional=True)
  )
  (doc_self_attn): AttentionWrapper(
    (score_func): SimilarityWrapper(
      (score_func): DotProductProject(
        (dropout): DropoutWrapper()
        (proj_1): Linear(in_features=2436, out_features=128, bias=False)
        (proj_2): Linear(in_features=2436, out_features=128, bias=False)
      )
    )
  )
  (doc_mem_gen): OneLayerBRNN(
    (dropout): DropoutWrapper()
    (rnn): LSTM(512, 128, bidirectional=True)
  )
  (query_domain_mem): OneLayerBRNN(
    (dropout): DropoutWrapper()
    (rnn): LSTM(768, 128, bidirectional=True)
  )
  (query_sum_attn): SelfAttnWrapper(
    (att): LinearSelfAttn(
      (linear): Linear(in_features=256, out_features=1, bias=True)
      (dropout): DropoutWrapper()
    )
  )
  (decoder): SAN(
    (attn_b): FlatSimilarityWrapper(
      (att_dropout): DropoutWrapper()
      (score_func): BilinearFlatSim(
        (linear): Linear(in_features=256, out_features=256, bias=True)
        (dropout): DropoutWrapper()
      )
    )
    (attn_e): FlatSimilarityWrapper(
      (att_dropout): DropoutWrapper()
      (score_func): BilinearFlatSim(
        (linear): Linear(in_features=256, out_features=256, bias=True)
        (dropout): DropoutWrapper()
      )
    )
    (rnn): GRUCell(256, 256)
    (dropout): DropoutWrapper()
  )
  (doc_sum_attn): SelfAttnWrapper(
    (att): LinearSelfAttn(
      (linear): Linear(in_features=256, out_features=1, bias=True)
      (dropout): DropoutWrapper()
    )
  )
  (classifier): Classifier(
    (dropout): DropoutWrapper()
    (proj): Linear(in_features=512, out_features=1, bias=True)
  )
)
]
2018-11-25 10:06:08 INFO     [Total number of params: 10114357]
2018-11-25 10:06:13 WARNING  [At epoch 0]
2018-11-25 10:06:13 INFO     [#updates[     1] train loss[10.57590] remaining[0:17:08]]
2018-11-25 10:06:42 INFO     [#updates[   100] train loss[6.28529] remaining[0:19:11]]
2018-11-25 10:07:13 INFO     [#updates[   200] train loss[5.91765] remaining[0:19:21]]
2018-11-25 10:07:40 INFO     [#updates[   300] train loss[5.74635] remaining[0:18:20]]
2018-11-25 10:08:10 INFO     [#updates[   400] train loss[5.59522] remaining[0:17:57]]
2018-11-25 10:08:42 INFO     [#updates[   500] train loss[5.39942] remaining[0:17:44]]
2018-11-25 10:09:12 INFO     [#updates[   600] train loss[5.31564] remaining[0:17:16]]
2018-11-25 10:09:40 INFO     [#updates[   700] train loss[5.24065] remaining[0:16:36]]
2018-11-25 10:10:10 INFO     [#updates[   800] train loss[5.14362] remaining[0:16:10]]
2018-11-25 10:10:37 INFO     [#updates[   900] train loss[5.07855] remaining[0:15:31]]
2018-11-25 10:11:04 INFO     [#updates[  1000] train loss[5.02174] remaining[0:14:54]]
2018-11-25 10:11:32 INFO     [#updates[  1100] train loss[4.95941] remaining[0:14:22]]
2018-11-25 10:11:57 INFO     [#updates[  1200] train loss[4.94419] remaining[0:13:44]]
2018-11-25 10:12:24 INFO     [#updates[  1300] train loss[4.91934] remaining[0:13:11]]
2018-11-25 10:12:51 INFO     [#updates[  1400] train loss[4.85124] remaining[0:12:39]]
2018-11-25 10:13:20 INFO     [#updates[  1500] train loss[4.81971] remaining[0:12:13]]
2018-11-25 10:13:49 INFO     [#updates[  1600] train loss[4.77904] remaining[0:11:44]]
2018-11-25 10:14:19 INFO     [#updates[  1700] train loss[4.75092] remaining[0:11:18]]
2018-11-25 10:14:49 INFO     [#updates[  1800] train loss[4.72823] remaining[0:10:51]]
2018-11-25 10:15:17 INFO     [#updates[  1900] train loss[4.71418] remaining[0:10:22]]
2018-11-25 10:15:49 INFO     [#updates[  2000] train loss[4.69012] remaining[0:09:57]]
2018-11-25 10:16:16 INFO     [#updates[  2100] train loss[4.66242] remaining[0:09:27]]
2018-11-25 10:16:47 INFO     [#updates[  2200] train loss[4.64709] remaining[0:08:59]]
2018-11-25 10:17:15 INFO     [#updates[  2300] train loss[4.63226] remaining[0:08:30]]
2018-11-25 10:17:43 INFO     [#updates[  2400] train loss[4.62183] remaining[0:08:00]]
2018-11-25 10:18:08 INFO     [#updates[  2500] train loss[4.61771] remaining[0:07:30]]
2018-11-25 10:18:28 INFO     [#updates[  2600] train loss[4.59929] remaining[0:06:56]]
2018-11-25 10:18:48 INFO     [#updates[  2700] train loss[4.57354] remaining[0:06:24]]
2018-11-25 10:19:09 INFO     [#updates[  2800] train loss[4.54780] remaining[0:05:53]]
2018-11-25 10:19:30 INFO     [#updates[  2900] train loss[4.54078] remaining[0:05:22]]
2018-11-25 10:19:51 INFO     [#updates[  3000] train loss[4.52369] remaining[0:04:52]]
2018-11-25 10:20:12 INFO     [#updates[  3100] train loss[4.49818] remaining[0:04:23]]
2018-11-25 10:20:32 INFO     [#updates[  3200] train loss[4.47941] remaining[0:03:54]]
2018-11-25 10:20:53 INFO     [#updates[  3300] train loss[4.46487] remaining[0:03:26]]
2018-11-25 10:21:14 INFO     [#updates[  3400] train loss[4.45162] remaining[0:02:58]]
2018-11-25 10:21:35 INFO     [#updates[  3500] train loss[4.43794] remaining[0:02:30]]
2018-11-25 10:21:56 INFO     [#updates[  3600] train loss[4.42402] remaining[0:02:03]]
2018-11-25 10:22:21 INFO     [#updates[  3700] train loss[4.41132] remaining[0:01:37]]
2018-11-25 10:22:51 INFO     [#updates[  3800] train loss[4.39404] remaining[0:01:11]]
2018-11-25 10:23:23 INFO     [#updates[  3900] train loss[4.38044] remaining[0:00:45]]
2018-11-25 10:23:52 INFO     [#updates[  4000] train loss[4.36391] remaining[0:00:19]]
2018-11-25 10:24:58 INFO     [scheduler_type ms]
2018-11-25 10:25:02 INFO     [Saved the new best model and prediction]
2018-11-25 10:25:02 WARNING  [Epoch 0 - dev EM: 21.511 F1: 27.001 (best EM: 21.511 F1: 27.001)]
2018-11-25 10:25:02 WARNING  [Epoch 0 - ACC: 49.9284]
2018-11-25 10:25:02 WARNING  [Detailed Metric at Epoch 0: OrderedDict([('exact', 21.51099132485471), ('f1', 27.000697304333492), ('total', 11873), ('HasAns_exact', 43.016194331983804), ('HasAns_f1', 54.011349374890614), ('HasAns_total', 5928), ('NoAns_exact', 0.0672834314550042), ('NoAns_f1', 0.0672834314550042), ('NoAns_total', 5945)])]
2018-11-25 10:25:02 WARNING  [At epoch 1]
2018-11-25 10:25:02 INFO     [#updates[  4074] train loss[4.35284] remaining[0:12:44]]
2018-11-25 10:25:10 INFO     [#updates[  4100] train loss[4.34484] remaining[0:20:48]]
2018-11-25 10:25:40 INFO     [#updates[  4200] train loss[4.32551] remaining[0:19:46]]
2018-11-25 10:26:07 INFO     [#updates[  4300] train loss[4.30895] remaining[0:18:22]]
2018-11-25 10:26:39 INFO     [#updates[  4400] train loss[4.28998] remaining[0:18:37]]
2018-11-25 10:27:11 INFO     [#updates[  4500] train loss[4.27375] remaining[0:18:23]]
2018-11-25 10:27:41 INFO     [#updates[  4600] train loss[4.25416] remaining[0:17:53]]
2018-11-25 10:28:08 INFO     [#updates[  4700] train loss[4.23725] remaining[0:17:06]]
2018-11-25 10:28:42 INFO     [#updates[  4800] train loss[4.21272] remaining[0:16:52]]
2018-11-25 10:29:12 INFO     [#updates[  4900] train loss[4.19264] remaining[0:16:22]]
2018-11-25 10:29:40 INFO     [#updates[  5000] train loss[4.16750] remaining[0:15:45]]
2018-11-25 10:30:11 INFO     [#updates[  5100] train loss[4.15236] remaining[0:15:16]]
2018-11-25 10:30:38 INFO     [#updates[  5200] train loss[4.13158] remaining[0:14:38]]
2018-11-25 10:31:07 INFO     [#updates[  5300] train loss[4.12009] remaining[0:14:08]]
2018-11-25 10:31:39 INFO     [#updates[  5400] train loss[4.10116] remaining[0:13:43]]
2018-11-25 10:32:09 INFO     [#updates[  5500] train loss[4.08220] remaining[0:13:11]]
2018-11-25 10:32:40 INFO     [#updates[  5600] train loss[4.06169] remaining[0:12:44]]
2018-11-25 10:33:13 INFO     [#updates[  5700] train loss[4.04193] remaining[0:12:18]]
2018-11-25 10:33:43 INFO     [#updates[  5800] train loss[4.02319] remaining[0:11:48]]
2018-11-25 10:34:12 INFO     [#updates[  5900] train loss[4.00372] remaining[0:11:16]]
2018-11-25 10:34:45 INFO     [#updates[  6000] train loss[3.98457] remaining[0:10:50]]
2018-11-25 10:35:18 INFO     [#updates[  6100] train loss[3.97011] remaining[0:10:22]]
2018-11-25 10:35:51 INFO     [#updates[  6200] train loss[3.95711] remaining[0:09:54]]
2018-11-25 10:36:21 INFO     [#updates[  6300] train loss[3.94049] remaining[0:09:23]]
2018-11-25 10:36:50 INFO     [#updates[  6400] train loss[3.92338] remaining[0:08:51]]
2018-11-25 10:37:21 INFO     [#updates[  6500] train loss[3.90604] remaining[0:08:21]]
2018-11-25 10:37:50 INFO     [#updates[  6600] train loss[3.89140] remaining[0:07:50]]
2018-11-25 10:38:19 INFO     [#updates[  6700] train loss[3.87150] remaining[0:07:18]]
2018-11-25 10:38:50 INFO     [#updates[  6800] train loss[3.85383] remaining[0:06:48]]
2018-11-25 10:39:19 INFO     [#updates[  6900] train loss[3.83908] remaining[0:06:17]]
2018-11-25 10:39:51 INFO     [#updates[  7000] train loss[3.82645] remaining[0:05:48]]
2018-11-25 10:40:23 INFO     [#updates[  7100] train loss[3.81335] remaining[0:05:18]]
2018-11-25 10:40:51 INFO     [#updates[  7200] train loss[3.79911] remaining[0:04:47]]
2018-11-25 10:41:18 INFO     [#updates[  7300] train loss[3.78708] remaining[0:04:15]]
2018-11-25 10:41:48 INFO     [#updates[  7400] train loss[3.77354] remaining[0:03:45]]
2018-11-25 10:42:18 INFO     [#updates[  7500] train loss[3.75798] remaining[0:03:15]]
2018-11-25 10:42:49 INFO     [#updates[  7600] train loss[3.74485] remaining[0:02:45]]
2018-11-25 10:43:20 INFO     [#updates[  7700] train loss[3.73244] remaining[0:02:15]]
2018-11-25 10:43:53 INFO     [#updates[  7800] train loss[3.72127] remaining[0:01:45]]
2018-11-25 10:44:23 INFO     [#updates[  7900] train loss[3.70737] remaining[0:01:14]]
2018-11-25 10:44:55 INFO     [#updates[  8000] train loss[3.69431] remaining[0:00:44]]
2018-11-25 10:45:27 INFO     [#updates[  8100] train loss[3.68078] remaining[0:00:13]]
2018-11-25 10:46:26 INFO     [scheduler_type ms]
2018-11-25 10:46:29 INFO     [Saved the new best model and prediction]
2018-11-25 10:46:29 WARNING  [Epoch 1 - dev EM: 49.912 F1: 50.033 (best EM: 49.912 F1: 50.033)]
2018-11-25 10:46:29 WARNING  [Epoch 1 - ACC: 50.5096]
2018-11-25 10:46:29 WARNING  [Detailed Metric at Epoch 1: OrderedDict([('exact', 49.91156405289312), ('f1', 50.03323998050799), ('total', 11873), ('HasAns_exact', 1.3326585695006747), ('HasAns_f1', 1.5763593604202653), ('HasAns_total', 5928), ('NoAns_exact', 98.3515559293524), ('NoAns_f1', 98.3515559293524), ('NoAns_total', 5945)])]
2018-11-25 10:46:29 WARNING  [At epoch 2]
2018-11-25 10:46:29 INFO     [#updates[  8147] train loss[3.67466] remaining[0:11:14]]
2018-11-25 10:46:47 INFO     [#updates[  8200] train loss[3.66752] remaining[0:22:50]]
2018-11-25 10:47:18 INFO     [#updates[  8300] train loss[3.65486] remaining[0:20:54]]
2018-11-25 10:47:50 INFO     [#updates[  8400] train loss[3.64276] remaining[0:20:13]]
2018-11-25 10:48:19 INFO     [#updates[  8500] train loss[3.62887] remaining[0:19:10]]
2018-11-25 10:48:49 INFO     [#updates[  8600] train loss[3.61956] remaining[0:18:34]]
2018-11-25 10:49:18 INFO     [#updates[  8700] train loss[3.60512] remaining[0:17:52]]
2018-11-25 10:49:50 INFO     [#updates[  8800] train loss[3.59217] remaining[0:17:31]]
2018-11-25 10:50:20 INFO     [#updates[  8900] train loss[3.58156] remaining[0:16:55]]
2018-11-25 10:50:50 INFO     [#updates[  9000] train loss[3.56989] remaining[0:16:22]]
2018-11-25 10:51:20 INFO     [#updates[  9100] train loss[3.55855] remaining[0:15:51]]
2018-11-25 10:51:48 INFO     [#updates[  9200] train loss[3.54666] remaining[0:15:14]]
2018-11-25 10:52:15 INFO     [#updates[  9300] train loss[3.53493] remaining[0:14:36]]
2018-11-25 10:52:47 INFO     [#updates[  9400] train loss[3.52374] remaining[0:14:09]]
2018-11-25 10:53:19 INFO     [#updates[  9500] train loss[3.51087] remaining[0:13:44]]
2018-11-25 10:53:51 INFO     [#updates[  9600] train loss[3.49991] remaining[0:13:16]]
2018-11-25 10:54:23 INFO     [#updates[  9700] train loss[3.49155] remaining[0:12:48]]
2018-11-25 10:54:56 INFO     [#updates[  9800] train loss[3.48075] remaining[0:12:21]]
2018-11-25 10:55:28 INFO     [#updates[  9900] train loss[3.47064] remaining[0:11:52]]
2018-11-25 10:56:00 INFO     [#updates[ 10000] train loss[3.46047] remaining[0:11:23]]
2018-11-25 10:56:32 INFO     [#updates[ 10100] train loss[3.45064] remaining[0:10:53]]
2018-11-25 10:57:03 INFO     [#updates[ 10200] train loss[3.44132] remaining[0:10:23]]
2018-11-25 10:57:34 INFO     [#updates[ 10300] train loss[3.43243] remaining[0:09:52]]
2018-11-25 10:58:03 INFO     [#updates[ 10400] train loss[3.42289] remaining[0:09:19]]
2018-11-25 10:58:35 INFO     [#updates[ 10500] train loss[3.41387] remaining[0:08:49]]
2018-11-25 10:59:06 INFO     [#updates[ 10600] train loss[3.40573] remaining[0:08:19]]
2018-11-25 10:59:41 INFO     [#updates[ 10700] train loss[3.39634] remaining[0:07:51]]
2018-11-25 11:00:12 INFO     [#updates[ 10800] train loss[3.38819] remaining[0:07:20]]
2018-11-25 11:00:44 INFO     [#updates[ 10900] train loss[3.37943] remaining[0:06:49]]
2018-11-25 11:01:14 INFO     [#updates[ 11000] train loss[3.36905] remaining[0:06:18]]
2018-11-25 11:01:45 INFO     [#updates[ 11100] train loss[3.35794] remaining[0:05:47]]
2018-11-25 11:02:16 INFO     [#updates[ 11200] train loss[3.35054] remaining[0:05:16]]
2018-11-25 11:02:47 INFO     [#updates[ 11300] train loss[3.34110] remaining[0:04:44]]

You may want to consider changing your batch submission script as follows to speed up your job run next time:

#PBS -l select=1:ncpus=28:mem=6gb
#PBS -l place=free:shared
#PBS -l walltime=01:02:00

Your group changxuwu has been charged 00:57:28 for 28 cpus.
You previously had 22584:45:48.  You now have 22557:56:44 remaining for the queue oc_standard
